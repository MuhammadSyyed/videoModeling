{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30d2a437",
   "metadata": {},
   "source": [
    "# Task 1: Video Action Recognition using Spatio-Temporal Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92d7a699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "# !pip install opencv-python tqdm matplotlib ffmpeg-python decord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f6b3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torchvision\n",
    "from glob import glob\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as T\n",
    "import torchvision.io as io\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import os\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "from utils import save_checkpoint, plot_metrics, visualize_activation_on_frames\n",
    "from dataset import VideoFolderDataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b055951",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd47b82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def uniform_sample_indices(num_frames, clip_len):\n",
    "    if num_frames <= clip_len:\n",
    "        # pad by repeating last frame\n",
    "        repeats = list(range(num_frames)) + [num_frames-1]*(clip_len - num_frames)\n",
    "        return repeats[:clip_len]\n",
    "    interval = num_frames / float(clip_len)\n",
    "    return [int(i * interval) for i in range(clip_len)]\n",
    "\n",
    "class VideoFolderDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Expects dataset_root/class_name/*.mp4\n",
    "    Returns: tensor (C,T,H,W), label\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir, clip_len=16, transforms=None, extensions=('.mp4','.avi','.mov')):\n",
    "        self.root_dir = root_dir\n",
    "        self.clip_len = clip_len\n",
    "        self.extensions = extensions\n",
    "        self.transforms = transforms\n",
    "        self.samples = []\n",
    "        cls_dirs = sorted([d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir,d))])\n",
    "        self.class_to_idx = {c:i for i,c in enumerate(cls_dirs)}\n",
    "        for c in cls_dirs:\n",
    "            p = os.path.join(root_dir, c)\n",
    "            for ext in extensions:\n",
    "                for f in glob(os.path.join(p, f\"**/*{ext}\"), recursive=True):\n",
    "                    self.samples.append((f, self.class_to_idx[c]))\n",
    "        if len(self.samples)==0:\n",
    "            raise RuntimeError(f\"No videos found in {root_dir} with extensions {extensions}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def read_video_rgb(self, path):\n",
    "        # returns (T, H, W, C) numpy\n",
    "        video, _, info = io.read_video(path, pts_unit='sec')  # (T, H, W, C)\n",
    "        if isinstance(video, torch.Tensor):\n",
    "            video = video.numpy()\n",
    "        return video  # dtype uint8\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.samples[idx]\n",
    "        video = self.read_video_rgb(path)  # (T,H,W,C) uint8\n",
    "        num_frames = video.shape[0]\n",
    "        indices = uniform_sample_indices(num_frames, self.clip_len)\n",
    "        frames = video[indices]  # (clip_len,H,W,C)\n",
    "        # convert to PIL per-frame and apply transforms if provided\n",
    "        pil_frames = [Image.fromarray(fr) for fr in frames]\n",
    "        if self.transforms:\n",
    "            # apply same transform to each frame\n",
    "            pil_frames = [self.transforms(f) for f in pil_frames]  # each -> tensor C,H,W\n",
    "        else:\n",
    "            # default convert to tensor and permute\n",
    "            pil_frames = [T.ToTensor()(f) for f in pil_frames]\n",
    "\n",
    "        # stack to shape (T, C, H, W) then permute to (C, T, H, W)\n",
    "        frame_tensors = torch.stack(pil_frames)  # (T, C, H, W)\n",
    "        frame_tensors = frame_tensors.permute(1, 0, 2, 3).contiguous()  # (C, T, H, W)\n",
    "        return frame_tensors.float(), label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c65af0",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cb32d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, out_dir, name='checkpoint.pth'):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    path = os.path.join(out_dir, name)\n",
    "    torch.save(state, path)\n",
    "    if is_best:\n",
    "        best_path = os.path.join(out_dir, 'model_best.pth')\n",
    "        torch.save(state, best_path)\n",
    "\n",
    "def plot_metrics(train_losses, val_losses, train_accs, val_accs, out_dir):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.plot(train_losses, label='train_loss')\n",
    "    plt.plot(val_losses, label='val_loss')\n",
    "    plt.legend()\n",
    "    plt.title('Loss')\n",
    "    plt.savefig(os.path.join(out_dir, 'loss_curve.png'))\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.plot(train_accs, label='train_acc')\n",
    "    plt.plot(val_accs, label='val_acc')\n",
    "    plt.legend()\n",
    "    plt.title('Top-1 Accuracy')\n",
    "    plt.savefig(os.path.join(out_dir, 'acc_curve.png'))\n",
    "    plt.close()\n",
    "\n",
    "def visualize_activation_on_frames(frames_tensor, activation_map, out_path_prefix):\n",
    "    \"\"\"\n",
    "    frames_tensor: (T, C, H, W) in [0,1] or [0,255]\n",
    "    activation_map: (T, H, W) or (H, W) that aligns with frames\n",
    "    Saves overlay images for each frame (simple grayscale heatmap overlay).\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    from matplotlib import cm\n",
    "    os.makedirs(os.path.dirname(out_path_prefix), exist_ok=True)\n",
    "    T = frames_tensor.shape[0]\n",
    "    for t in range(T):\n",
    "        frame = frames_tensor[t].permute(1,2,0).cpu().numpy()  # H,W,C\n",
    "        if frame.max()>2:\n",
    "            frame = frame/255.0\n",
    "        act = activation_map[t] if activation_map.ndim==3 or activation_map.shape[0]==T else activation_map\n",
    "        act = act if act.shape==(frame.shape[0], frame.shape[1]) else \\\n",
    "              np.array(Image.fromarray(act).resize((frame.shape[1], frame.shape[0])))\n",
    "        plt.figure(figsize=(4,4))\n",
    "        plt.imshow(frame)\n",
    "        plt.imshow(act, cmap='jet', alpha=0.4)\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{out_path_prefix}_frame{t:02d}.png\")\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34a8ccb",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201bf605",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(side_length=112):\n",
    "    return T.Compose([\n",
    "        T.Resize((side_length, side_length)),\n",
    "        T.CenterCrop(side_length),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(\n",
    "            mean=[0.43216, 0.394666, 0.37645],\n",
    "            std=[0.22803, 0.22145, 0.216989]\n",
    "        ),\n",
    "    ])\n",
    "\n",
    "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for x, y in tqdm(loader, desc='Train', leave=False):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * x.size(0)\n",
    "        correct += (out.argmax(1) == y).sum().item()\n",
    "        total += x.size(0)\n",
    "\n",
    "    return running_loss / total, correct / total\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for x, y in tqdm(loader, desc='Val', leave=False):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "\n",
    "        running_loss += loss.item() * x.size(0)\n",
    "        correct += (out.argmax(1) == y).sum().item()\n",
    "        total += x.size(0)\n",
    "\n",
    "    return running_loss / total, correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a409a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIG ===\n",
    "data_root   = \"/workspace/KineticsMini/train\"   # CHANGE THIS\n",
    "out_dir     = \"outputs_task1\"\n",
    "clip_len    = 16\n",
    "batch_size  = 8\n",
    "epochs      = 10\n",
    "lr          = 1e-4\n",
    "side_len    = 112\n",
    "num_workers = 4\n",
    "device      = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829a5f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = get_transforms(side_len)\n",
    "\n",
    "dataset = VideoFolderDataset(\n",
    "    data_root,\n",
    "    clip_len=clip_len,\n",
    "    transforms=transforms\n",
    ")\n",
    "\n",
    "num_classes = len(dataset.class_to_idx)\n",
    "print(\"Classes:\", dataset.class_to_idx)\n",
    "\n",
    "# Split\n",
    "n_val = int(0.2 * len(dataset))\n",
    "n_train = len(dataset) - n_val\n",
    "train_set, val_set = random_split(dataset, [n_train, n_val])\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "val_loader   = DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19adbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.video.r2plus1d_18(pretrained=False)\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "train_accs, val_accs = [], []\n",
    "best_acc = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329d48ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    print(f\"Epoch {epoch}/{epochs}\")\n",
    "\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    val_accs.append(val_acc)\n",
    "\n",
    "    is_best = val_acc > best_acc\n",
    "    best_acc = max(best_acc, val_acc)\n",
    "\n",
    "    save_checkpoint(\n",
    "        {\n",
    "            'epoch': epoch,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'best_acc': best_acc,\n",
    "            'class_to_idx': dataset.class_to_idx\n",
    "        },\n",
    "        is_best,\n",
    "        out_dir,\n",
    "        name=f\"checkpoint_epoch{epoch}.pth\"\n",
    "    )\n",
    "\n",
    "    plot_metrics(train_losses, val_losses, train_accs, val_accs, out_dir)\n",
    "\n",
    "    print(f\"Train Loss {train_loss:.4f} Acc {train_acc:.4f}\")\n",
    "    print(f\"Val   Loss {val_loss:.4f} Acc {val_acc:.4f}  BEST {best_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b3e809",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "sample_x, _ = dataset[0]\n",
    "sample_x = sample_x.unsqueeze(0).to(device)\n",
    "\n",
    "activations = {}\n",
    "\n",
    "def hook_fn(module, inp, outp):\n",
    "    activations['feat'] = outp.detach().cpu()\n",
    "\n",
    "handle = model.layer4[0].conv1.register_forward_hook(hook_fn)\n",
    "_ = model(sample_x)\n",
    "handle.remove()\n",
    "\n",
    "feat = activations['feat']  # (1, C, T', H', W')\n",
    "feat_mean = feat.squeeze(0).mean(0).numpy()  # (T', H', W')\n",
    "\n",
    "up_acts = []\n",
    "T_feat = feat_mean.shape[0]\n",
    "\n",
    "for t in range(T_feat):\n",
    "    a = feat_mean[t]\n",
    "    resized = cv2.resize(a, (sample_x.shape[4], sample_x.shape[3]))\n",
    "    up_acts.append(resized)\n",
    "\n",
    "frames = sample_x.squeeze(0).permute(1,0,2,3).cpu()\n",
    "visualize_activation_on_frames(frames, np.stack(up_acts), os.path.join(out_dir, \"activation\"))\n",
    "\n",
    "print(\"Activation visualizations saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
