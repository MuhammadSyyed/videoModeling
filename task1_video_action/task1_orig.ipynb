{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30d2a437",
   "metadata": {},
   "source": [
    "# Task 1: Video Action Recognition using Spatio-Temporal Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92d7a699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "# !pip install opencv-python tqdm matplotlib ffmpeg-python decord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fc6c16e-1d4b-43ca-a8d9-f2140e4156a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install av"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "362bbb7e-3743-4ca7-b914-6604d02ab56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/.cache/kagglehub/datasets/matthewjansen/ucf101-action-recognition/versions/4\n"
     ]
    }
   ],
   "source": [
    "# import kagglehub\n",
    "\n",
    "# # Download latest version\n",
    "# path = kagglehub.dataset_download(\"matthewjansen/ucf101-action-recognition\")\n",
    "\n",
    "# print(\"Path to dataset files:\", path)\n",
    "\n",
    "print(\"/root/.cache/kagglehub/datasets/matthewjansen/ucf101-action-recognition/versions/4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ade0283-b93b-4e7f-8c2a-83f47c812eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install numpy==1.26.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67f6b3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import av\n",
    "import random\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as T\n",
    "import torchvision.io as io\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import os\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b055951",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd47b82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def uniform_sample_indices(num_frames, clip_len):\n",
    "    if num_frames <= clip_len:\n",
    "        # pad by repeating last frame\n",
    "        repeats = list(range(num_frames)) + [num_frames-1]*(clip_len - num_frames)\n",
    "        return repeats[:clip_len]\n",
    "    interval = num_frames / float(clip_len)\n",
    "    return [int(i * interval) for i in range(clip_len)]\n",
    "\n",
    "class VideoFolderDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Expects dataset_root/class_name/*.mp4\n",
    "    Returns: tensor (C,T,H,W), label\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir, clip_len=16, transforms=None, extensions=('.mp4','.avi','.mov')):\n",
    "        self.root_dir = root_dir\n",
    "        self.clip_len = clip_len\n",
    "        self.extensions = extensions\n",
    "        self.transforms = transforms\n",
    "        self.samples = []\n",
    "        cls_dirs = sorted([d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir,d))])\n",
    "        self.class_to_idx = {c:i for i,c in enumerate(cls_dirs)}\n",
    "        for c in cls_dirs:\n",
    "            p = os.path.join(root_dir, c)\n",
    "            for ext in extensions:\n",
    "                for f in glob(os.path.join(p, f\"**/*{ext}\"), recursive=True):\n",
    "                    self.samples.append((f, self.class_to_idx[c]))\n",
    "        if len(self.samples)==0:\n",
    "            raise RuntimeError(f\"No videos found in {root_dir} with extensions {extensions}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def read_video_rgb(self, path):\n",
    "        # returns (T, H, W, C) numpy\n",
    "        video, _, info = io.read_video(path, pts_unit='sec')  # (T, H, W, C)\n",
    "        if isinstance(video, torch.Tensor):\n",
    "            video = video.numpy()\n",
    "        return video  # dtype uint8\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.samples[idx]\n",
    "        video = self.read_video_rgb(path)  # (T,H,W,C) uint8\n",
    "        num_frames = video.shape[0]\n",
    "        indices = uniform_sample_indices(num_frames, self.clip_len)\n",
    "        frames = video[indices]  # (clip_len,H,W,C)\n",
    "        # convert to PIL per-frame and apply transforms if provided\n",
    "        pil_frames = [Image.fromarray(fr) for fr in frames]\n",
    "        if self.transforms:\n",
    "            # apply same transform to each frame\n",
    "            pil_frames = [self.transforms(f) for f in pil_frames]  # each -> tensor C,H,W\n",
    "        else:\n",
    "            # default convert to tensor and permute\n",
    "            pil_frames = [T.ToTensor()(f) for f in pil_frames]\n",
    "\n",
    "        # stack to shape (T, C, H, W) then permute to (C, T, H, W)\n",
    "        frame_tensors = torch.stack(pil_frames)  # (T, C, H, W)\n",
    "        frame_tensors = frame_tensors.permute(1, 0, 2, 3).contiguous()  # (C, T, H, W)\n",
    "        return frame_tensors.float(), label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c65af0",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7cb32d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, out_dir, name='checkpoint.pth'):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    path = os.path.join(out_dir, name)\n",
    "    torch.save(state, path)\n",
    "    if is_best:\n",
    "        best_path = os.path.join(out_dir, 'model_best.pth')\n",
    "        torch.save(state, best_path)\n",
    "\n",
    "def plot_metrics(train_losses, val_losses, train_accs, val_accs, out_dir):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.plot(train_losses, label='train_loss')\n",
    "    plt.plot(val_losses, label='val_loss')\n",
    "    plt.legend()\n",
    "    plt.title('Loss')\n",
    "    plt.savefig(os.path.join(out_dir, 'loss_curve.png'))\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.plot(train_accs, label='train_acc')\n",
    "    plt.plot(val_accs, label='val_acc')\n",
    "    plt.legend()\n",
    "    plt.title('Top-1 Accuracy')\n",
    "    plt.savefig(os.path.join(out_dir, 'acc_curve.png'))\n",
    "    plt.close()\n",
    "\n",
    "def visualize_activation_on_frames(frames_tensor, activation_map, out_path_prefix):\n",
    "    \"\"\"\n",
    "    frames_tensor: (T, C, H, W) in [0,1] or [0,255]\n",
    "    activation_map: (T, H, W) or (H, W) that aligns with frames\n",
    "    Saves overlay images for each frame (simple grayscale heatmap overlay).\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    from matplotlib import cm\n",
    "    os.makedirs(os.path.dirname(out_path_prefix), exist_ok=True)\n",
    "    T = frames_tensor.shape[0]\n",
    "    for t in range(T):\n",
    "        frame = frames_tensor[t].permute(1,2,0).cpu().numpy()  # H,W,C\n",
    "        if frame.max()>2:\n",
    "            frame = frame/255.0\n",
    "        act = activation_map[t] if activation_map.ndim==3 or activation_map.shape[0]==T else activation_map\n",
    "        act = act if act.shape==(frame.shape[0], frame.shape[1]) else \\\n",
    "              np.array(Image.fromarray(act).resize((frame.shape[1], frame.shape[0])))\n",
    "        plt.figure(figsize=(4,4))\n",
    "        plt.imshow(frame)\n",
    "        plt.imshow(act, cmap='jet', alpha=0.4)\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{out_path_prefix}_frame{t:02d}.png\")\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34a8ccb",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "201bf605",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(side_length=112):\n",
    "    return T.Compose([\n",
    "        T.Resize((side_length, side_length)),\n",
    "        T.CenterCrop(side_length),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(\n",
    "            mean=[0.43216, 0.394666, 0.37645],\n",
    "            std=[0.22803, 0.22145, 0.216989]\n",
    "        ),\n",
    "    ])\n",
    "\n",
    "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for x, y in tqdm(loader, desc='Train', leave=False):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * x.size(0)\n",
    "        correct += (out.argmax(1) == y).sum().item()\n",
    "        total += x.size(0)\n",
    "\n",
    "    return running_loss / total, correct / total\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for x, y in tqdm(loader, desc='Val', leave=False):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "\n",
    "        running_loss += loss.item() * x.size(0)\n",
    "        correct += (out.argmax(1) == y).sum().item()\n",
    "        total += x.size(0)\n",
    "\n",
    "    return running_loss / total, correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a409a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# === CONFIG ===\n",
    "data_root = \"/workspace/videoModeling/task1_video_action/dataset/train\"\n",
    "out_dir     = \"outputs_task1\"\n",
    "clip_len    = 16\n",
    "batch_size  = 8\n",
    "epochs      = 10\n",
    "lr          = 1e-4\n",
    "side_len    = 112\n",
    "num_workers = 4\n",
    "device      = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e50d8e6-0e02-4caf-9c23-6c0ed237c874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import shutil\n",
    "\n",
    "# BASE = \"/root/.cache/kagglehub/datasets/matthewjansen/ucf101-action-recognition/versions/4\"\n",
    "# OUT_ROOT = \"/workspace/videoModeling/task1_video_action/dataset\"\n",
    "# os.makedirs(OUT_ROOT, exist_ok=True)\n",
    "\n",
    "# def process_split(split):\n",
    "#     print(f\"\\n=== Processing {split} split ===\")\n",
    "\n",
    "#     csv_path = os.path.join(BASE, f\"{split}.csv\")\n",
    "#     split_output_dir = os.path.join(OUT_ROOT, split)\n",
    "\n",
    "#     with open(csv_path, \"r\") as f:\n",
    "#         lines = f.readlines()\n",
    "\n",
    "#     # Skip header\n",
    "#     lines = lines[1:]\n",
    "\n",
    "#     for line in lines:\n",
    "#         parts = line.strip().split(\",\")\n",
    "\n",
    "#         # Skip corrupted or unexpected rows\n",
    "#         if len(parts) < 3:\n",
    "#             print(\"Skipping bad row:\", line)\n",
    "#             continue\n",
    "\n",
    "#         clip_name, clip_path, label = parts\n",
    "\n",
    "#         # fix leading slash\n",
    "#         clip_path = clip_path.lstrip(\"/\")\n",
    "\n",
    "#         # source: BASE/train/xxxx.avi\n",
    "#         src = os.path.join(BASE, clip_path)\n",
    "\n",
    "#         # destination: OUT_ROOT/train/label/xxxx.avi\n",
    "#         dst_dir = os.path.join(split_output_dir, label)\n",
    "#         os.makedirs(dst_dir, exist_ok=True)\n",
    "\n",
    "#         dst = os.path.join(dst_dir, os.path.basename(clip_path))\n",
    "\n",
    "#         if os.path.exists(src):\n",
    "#             shutil.copy(src, dst)\n",
    "#         else:\n",
    "#             print(\"Missing:\", src)\n",
    "\n",
    "# for split in [\"train\", \"val\", \"test\"]:\n",
    "#     process_split(split)\n",
    "\n",
    "# print(\"\\nðŸŽ‰ DONE! Dataset prepared successfully.\")\n",
    "# print(\"Path:\", OUT_ROOT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "829a5f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: {'ApplyEyeMakeup': 0, 'ApplyLipstick': 1, 'Archery': 2, 'BabyCrawling': 3, 'BalanceBeam': 4, 'BandMarching': 5, 'BaseballPitch': 6, 'Basketball': 7, 'BasketballDunk': 8, 'BenchPress': 9, 'Biking': 10, 'Billiards': 11, 'BlowDryHair': 12, 'BlowingCandles': 13, 'BodyWeightSquats': 14, 'Bowling': 15, 'BoxingPunchingBag': 16, 'BoxingSpeedBag': 17, 'BreastStroke': 18, 'BrushingTeeth': 19, 'CleanAndJerk': 20, 'CliffDiving': 21, 'CricketBowling': 22, 'CricketShot': 23, 'CuttingInKitchen': 24, 'Diving': 25, 'Drumming': 26, 'Fencing': 27, 'FieldHockeyPenalty': 28, 'FloorGymnastics': 29, 'FrisbeeCatch': 30, 'FrontCrawl': 31, 'GolfSwing': 32, 'Haircut': 33, 'HammerThrow': 34, 'Hammering': 35, 'HandstandPushups': 36, 'HandstandWalking': 37, 'HeadMassage': 38, 'HighJump': 39, 'HorseRace': 40, 'HorseRiding': 41, 'HulaHoop': 42, 'IceDancing': 43, 'JavelinThrow': 44, 'JugglingBalls': 45, 'JumpRope': 46, 'JumpingJack': 47, 'Kayaking': 48, 'Knitting': 49, 'LongJump': 50, 'Lunges': 51, 'MilitaryParade': 52, 'Mixing': 53, 'MoppingFloor': 54, 'Nunchucks': 55, 'ParallelBars': 56, 'PizzaTossing': 57, 'PlayingCello': 58, 'PlayingDaf': 59, 'PlayingDhol': 60, 'PlayingFlute': 61, 'PlayingGuitar': 62, 'PlayingPiano': 63, 'PlayingSitar': 64, 'PlayingTabla': 65, 'PlayingViolin': 66, 'PoleVault': 67, 'PommelHorse': 68, 'PullUps': 69, 'Punch': 70, 'PushUps': 71, 'Rafting': 72, 'RockClimbingIndoor': 73, 'RopeClimbing': 74, 'Rowing': 75, 'SalsaSpin': 76, 'ShavingBeard': 77, 'Shotput': 78, 'SkateBoarding': 79, 'Skiing': 80, 'Skijet': 81, 'SkyDiving': 82, 'SoccerJuggling': 83, 'SoccerPenalty': 84, 'StillRings': 85, 'SumoWrestling': 86, 'Surfing': 87, 'Swing': 88, 'TableTennisShot': 89, 'TaiChi': 90, 'TennisSwing': 91, 'ThrowDiscus': 92, 'TrampolineJumping': 93, 'Typing': 94, 'UnevenBars': 95, 'VolleyballSpiking': 96, 'WalkingWithDog': 97, 'WallPushups': 98, 'WritingOnBoard': 99, 'YoYo': 100}\n"
     ]
    }
   ],
   "source": [
    "transforms = get_transforms(side_len)\n",
    "\n",
    "dataset = VideoFolderDataset(\n",
    "    data_root,\n",
    "    clip_len=clip_len,\n",
    "    transforms=transforms\n",
    ")\n",
    "\n",
    "num_classes = len(dataset.class_to_idx)\n",
    "print(\"Classes:\", dataset.class_to_idx)\n",
    "\n",
    "# Split\n",
    "n_val = int(0.2 * len(dataset))\n",
    "n_train = len(dataset) - n_val\n",
    "train_set, val_set = random_split(dataset, [n_train, n_val])\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "val_loader   = DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d19adbcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = torchvision.models.video.r2plus1d_18(weights=False)\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "train_accs, val_accs = [], []\n",
    "best_acc = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "329d48ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss 4.0294 Acc 0.0939\n",
      "Val   Loss 3.5093 Acc 0.1581  BEST 0.1581\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss 3.2276 Acc 0.2108\n",
      "Val   Loss 2.9398 Acc 0.2481  BEST 0.2481\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss 2.7546 Acc 0.3092\n",
      "Val   Loss 2.4273 Acc 0.3526  BEST 0.3526\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss 2.2751 Acc 0.4161\n",
      "Val   Loss 2.0700 Acc 0.4525  BEST 0.4525\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss 1.8299 Acc 0.5246\n",
      "Val   Loss 1.7315 Acc 0.5231  BEST 0.5231\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss 1.4271 Acc 0.6366\n",
      "Val   Loss 1.5291 Acc 0.5728  BEST 0.5728\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss 1.0938 Acc 0.7185\n",
      "Val   Loss 1.2914 Acc 0.6440  BEST 0.6440\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss 0.7666 Acc 0.8062\n",
      "Val   Loss 1.2960 Acc 0.6390  BEST 0.6440\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss 0.5307 Acc 0.8687\n",
      "Val   Loss 1.1563 Acc 0.6832  BEST 0.6832\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss 0.3830 Acc 0.9091\n",
      "Val   Loss 1.4833 Acc 0.6280  BEST 0.6832\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    print(f\"Epoch {epoch}/{epochs}\")\n",
    "\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    val_accs.append(val_acc)\n",
    "\n",
    "    is_best = val_acc > best_acc\n",
    "    best_acc = max(best_acc, val_acc)\n",
    "\n",
    "    save_checkpoint(\n",
    "        {\n",
    "            'epoch': epoch,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'best_acc': best_acc,\n",
    "            'class_to_idx': dataset.class_to_idx\n",
    "        },\n",
    "        is_best,\n",
    "        out_dir,\n",
    "        name=f\"checkpoint_epoch{epoch}.pth\"\n",
    "    )\n",
    "\n",
    "    plot_metrics(train_losses, val_losses, train_accs, val_accs, out_dir)\n",
    "\n",
    "    print(f\"Train Loss {train_loss:.4f} Acc {train_acc:.4f}\")\n",
    "    print(f\"Val   Loss {val_loss:.4f} Acc {val_acc:.4f}  BEST {best_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1412879-14a6-4b2e-9045-0576fa0fde2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.0074321143..0.010631349].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.0074321143..0.010631349].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.0074321143..0.010773096].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.0074321143..0.010631349].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.0074321143..0.010702223].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.0074321143..0.0104896035].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.0074321143..0.010560476].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.0074321143..0.0104896035].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.0074321143..0.01041873].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.0074321143..0.01041873].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.0074321143..0.010347856].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.0074321143..0.01041873].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.0074321143..0.01041873].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.0074321143..0.010206111].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.0074321143..0.010206111].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.0074321143..0.0100643635].\n"
     ]
    }
   ],
   "source": [
    "acts = np.stack(up_acts)               # (T_feat, H, W)\n",
    "T_feat = acts.shape[0]\n",
    "T_orig = frames.shape[0]\n",
    "\n",
    "if T_feat != T_orig:\n",
    "    reps = int(np.ceil(T_orig / T_feat))\n",
    "    acts = np.repeat(acts, reps, axis=0)[:T_orig]\n",
    "\n",
    "visualize_activation_on_frames(frames, acts, os.path.join(out_dir, \"activation\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9b3e809",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.0074321143..0.010631349].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.0074321143..0.010631349].\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for axis 0 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m     up_acts\u001b[38;5;241m.\u001b[39mappend(resized)\n\u001b[1;32m     26\u001b[0m frames \u001b[38;5;241m=\u001b[39m sample_x\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[0;32m---> 27\u001b[0m \u001b[43mvisualize_activation_on_frames\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mup_acts\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mactivation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mActivation visualizations saved.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 41\u001b[0m, in \u001b[0;36mvisualize_activation_on_frames\u001b[0;34m(frames_tensor, activation_map, out_path_prefix)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m frame\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m     40\u001b[0m     frame \u001b[38;5;241m=\u001b[39m frame\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255.0\u001b[39m\n\u001b[0;32m---> 41\u001b[0m act \u001b[38;5;241m=\u001b[39m \u001b[43mactivation_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m activation_map\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m activation_map\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m==\u001b[39mT \u001b[38;5;28;01melse\u001b[39;00m activation_map\n\u001b[1;32m     42\u001b[0m act \u001b[38;5;241m=\u001b[39m act \u001b[38;5;28;01mif\u001b[39;00m act\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;241m==\u001b[39m(frame\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], frame\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01melse\u001b[39;00m \\\n\u001b[1;32m     43\u001b[0m       np\u001b[38;5;241m.\u001b[39marray(Image\u001b[38;5;241m.\u001b[39mfromarray(act)\u001b[38;5;241m.\u001b[39mresize((frame\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], frame\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])))\n\u001b[1;32m     44\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m4\u001b[39m))\n",
      "\u001b[0;31mIndexError\u001b[0m: index 2 is out of bounds for axis 0 with size 2"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "sample_x, _ = dataset[0]\n",
    "sample_x = sample_x.unsqueeze(0).to(device)\n",
    "\n",
    "activations = {}\n",
    "\n",
    "def hook_fn(module, inp, outp):\n",
    "    activations['feat'] = outp.detach().cpu()\n",
    "\n",
    "handle = model.layer4[0].conv1.register_forward_hook(hook_fn)\n",
    "_ = model(sample_x)\n",
    "handle.remove()\n",
    "\n",
    "feat = activations['feat']  # (1, C, T', H', W')\n",
    "feat_mean = feat.squeeze(0).mean(0).numpy()  # (T', H', W')\n",
    "\n",
    "up_acts = []\n",
    "T_feat = feat_mean.shape[0]\n",
    "\n",
    "for t in range(T_feat):\n",
    "    a = feat_mean[t]\n",
    "    resized = cv2.resize(a, (sample_x.shape[4], sample_x.shape[3]))\n",
    "    up_acts.append(resized)\n",
    "\n",
    "frames = sample_x.squeeze(0).permute(1,0,2,3).cpu()\n",
    "visualize_activation_on_frames(frames, np.stack(up_acts), os.path.join(out_dir, \"activation\"))\n",
    "\n",
    "print(\"Activation visualizations saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8da33088-1bd3-4c7e-a4a6-9a93452b8740",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'class_to_idx': dataset.class_to_idx\n",
    "}, os.path.join(out_dir, \"final_model.pth\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0a831146-eadc-4783-abf6-36d565174d43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VideoResNet(\n",
       "  (stem): R2Plus1dStem(\n",
       "    (0): Conv3d(3, 45, kernel_size=(1, 7, 7), stride=(1, 2, 2), padding=(0, 3, 3), bias=False)\n",
       "    (1): BatchNorm3d(45, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv3d(45, 64, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "    (4): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "  )\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2Plus1D(\n",
       "          (0): Conv3d(64, 144, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "          (1): BatchNorm3d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv3d(144, 64, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "        )\n",
       "        (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2Plus1D(\n",
       "          (0): Conv3d(64, 144, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "          (1): BatchNorm3d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv3d(144, 64, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "        )\n",
       "        (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2Plus1D(\n",
       "          (0): Conv3d(64, 144, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "          (1): BatchNorm3d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv3d(144, 64, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "        )\n",
       "        (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2Plus1D(\n",
       "          (0): Conv3d(64, 144, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "          (1): BatchNorm3d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv3d(144, 64, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "        )\n",
       "        (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2Plus1D(\n",
       "          (0): Conv3d(64, 230, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), bias=False)\n",
       "          (1): BatchNorm3d(230, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv3d(230, 128, kernel_size=(3, 1, 1), stride=(2, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "        )\n",
       "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2Plus1D(\n",
       "          (0): Conv3d(128, 230, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "          (1): BatchNorm3d(230, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv3d(230, 128, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "        )\n",
       "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
       "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2Plus1D(\n",
       "          (0): Conv3d(128, 288, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "          (1): BatchNorm3d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv3d(288, 128, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "        )\n",
       "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2Plus1D(\n",
       "          (0): Conv3d(128, 288, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "          (1): BatchNorm3d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv3d(288, 128, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "        )\n",
       "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2Plus1D(\n",
       "          (0): Conv3d(128, 460, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), bias=False)\n",
       "          (1): BatchNorm3d(460, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv3d(460, 256, kernel_size=(3, 1, 1), stride=(2, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "        )\n",
       "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2Plus1D(\n",
       "          (0): Conv3d(256, 460, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "          (1): BatchNorm3d(460, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv3d(460, 256, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "        )\n",
       "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv3d(128, 256, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
       "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2Plus1D(\n",
       "          (0): Conv3d(256, 576, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "          (1): BatchNorm3d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv3d(576, 256, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "        )\n",
       "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2Plus1D(\n",
       "          (0): Conv3d(256, 576, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "          (1): BatchNorm3d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv3d(576, 256, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "        )\n",
       "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2Plus1D(\n",
       "          (0): Conv3d(256, 921, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), bias=False)\n",
       "          (1): BatchNorm3d(921, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv3d(921, 512, kernel_size=(3, 1, 1), stride=(2, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "        )\n",
       "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2Plus1D(\n",
       "          (0): Conv3d(512, 921, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "          (1): BatchNorm3d(921, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv3d(921, 512, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "        )\n",
       "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv3d(256, 512, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
       "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2Plus1D(\n",
       "          (0): Conv3d(512, 1152, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "          (1): BatchNorm3d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv3d(1152, 512, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "        )\n",
       "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2Plus1D(\n",
       "          (0): Conv3d(512, 1152, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "          (1): BatchNorm3d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv3d(1152, 512, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "        )\n",
       "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool3d(output_size=(1, 1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=101, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torchvision.models.video.r2plus1d_18(pretrained=False)\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "checkpoint = torch.load(\"outputs_task1/final_model.pth\", map_location=device)\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "model = model.to(device)      # <--- IMPORTANT\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "52f69f5f-b7bc-4798-9510-b996d35f8f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(checkpoint[\"class_to_idx\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "682120af-52d2-4ec2-8c52-7ceed8bd9cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: ApplyEyeMakeup\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "sample_x, _ = dataset[10]     # or load your own clip\n",
    "sample_x = sample_x.unsqueeze(0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(sample_x)\n",
    "    pred_idx = logits.argmax(1).item()\n",
    "\n",
    "idx_to_class = {v: k for k, v in class_to_idx.items()}\n",
    "print(\"Predicted class:\", idx_to_class[pred_idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "903a45c4-9554-4387-a0fa-d5a4515148e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: ApplyEyeMakeup\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load model architecture\n",
    "model = torchvision.models.video.r2plus1d_18(pretrained=False)\n",
    "num_classes = len(class_to_idx)\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint = torch.load(\"outputs_task1/final_model.pth\", map_location=device)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "# Move model to GPU\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Prepare input\n",
    "sample_x, _ = dataset[0]\n",
    "sample_x = sample_x.unsqueeze(0).to(device)\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    logits = model(sample_x)\n",
    "    pred_idx = logits.argmax(1).item()\n",
    "\n",
    "idx_to_class = {v: k for k, v in class_to_idx.items()}\n",
    "print(\"Predicted class:\", idx_to_class[pred_idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ff5a8b-2865-4fa7-ba4d-7abf5e0399af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
