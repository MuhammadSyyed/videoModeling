{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b08b55de",
   "metadata": {},
   "source": [
    "# Task2: Vision Language Alingment with CLIP Style Contrastive Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e652320",
   "metadata": {},
   "source": [
    "## Installing libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df84803c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61001748",
   "metadata": {},
   "source": [
    "## Importing Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a372c118",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mT\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmodels\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmanifold\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TSNE\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpairwise\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cosine_similarity\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/videoModeling/.venv/lib/python3.11/site-packages/sklearn/manifold/__init__.py:6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"Data embedding techniques.\"\"\"\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Authors: The scikit-learn developers\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# SPDX-License-Identifier: BSD-3-Clause\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_isomap\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Isomap\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_locally_linear\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LocallyLinearEmbedding, locally_linear_embedding\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_mds\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MDS, smacof\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/videoModeling/.venv/lib/python3.11/site-packages/sklearn/manifold/_isomap.py:19\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msparse\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcsgraph\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m connected_components, shortest_path\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     14\u001b[39m     BaseEstimator,\n\u001b[32m     15\u001b[39m     ClassNamePrefixFeaturesOutMixin,\n\u001b[32m     16\u001b[39m     TransformerMixin,\n\u001b[32m     17\u001b[39m     _fit_context,\n\u001b[32m     18\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdecomposition\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KernelPCA\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpairwise\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _VALID_METRICS\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mneighbors\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NearestNeighbors, kneighbors_graph, radius_neighbors_graph\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/videoModeling/.venv/lib/python3.11/site-packages/sklearn/decomposition/__init__.py:23\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_incremental_pca\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m IncrementalPCA\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_kernel_pca\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KernelPCA\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_lda\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LatentDirichletAllocation\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_nmf\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     25\u001b[39m     NMF,\n\u001b[32m     26\u001b[39m     MiniBatchNMF,\n\u001b[32m     27\u001b[39m     non_negative_factorization,\n\u001b[32m     28\u001b[39m )\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_pca\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PCA\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/videoModeling/.venv/lib/python3.11/site-packages/sklearn/decomposition/_lda.py:31\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparallel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Parallel, delayed\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvalidation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m check_is_fitted, check_non_negative, validate_data\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_online_lda_fast\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     32\u001b[39m     _dirichlet_expectation_1d \u001b[38;5;28;01mas\u001b[39;00m cy_dirichlet_expectation_1d,\n\u001b[32m     33\u001b[39m )\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_online_lda_fast\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     35\u001b[39m     _dirichlet_expectation_2d,\n\u001b[32m     36\u001b[39m )\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_online_lda_fast\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     38\u001b[39m     mean_change \u001b[38;5;28;01mas\u001b[39;00m cy_mean_change,\n\u001b[32m     39\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:405\u001b[39m, in \u001b[36mparent\u001b[39m\u001b[34m(self)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mType:\u001b[39m        module\n",
      "\u001b[31mString form:\u001b[39m <module 'numpy' from '/Users/imranabbas/Documents/videoModeling/.venv/lib/python3.11/site-packages/numpy/__init__.py'>\n",
      "\u001b[31mFile:\u001b[39m        ~/Documents/videoModeling/.venv/lib/python3.11/site-packages/numpy/__init__.py\n",
      "\u001b[31mDocstring:\u001b[39m  \n",
      "NumPy\n",
      "=====\n",
      "\n",
      "Provides\n",
      "  1. An array object of arbitrary homogeneous items\n",
      "  2. Fast mathematical operations over arrays\n",
      "  3. Linear Algebra, Fourier Transforms, Random Number Generation\n",
      "\n",
      "How to use the documentation\n",
      "----------------------------\n",
      "Documentation is available in two forms: docstrings provided\n",
      "with the code, and a loose standing reference guide, available from\n",
      "`the NumPy homepage <https://numpy.org>`_.\n",
      "\n",
      "We recommend exploring the docstrings using\n",
      "`IPython <https://ipython.org>`_, an advanced Python shell with\n",
      "TAB-completion and introspection capabilities.  See below for further\n",
      "instructions.\n",
      "\n",
      "The docstring examples assume that `numpy` has been imported as ``np``::\n",
      "\n",
      "  >>> import numpy as np\n",
      "\n",
      "Code snippets are indicated by three greater-than signs::\n",
      "\n",
      "  >>> x = 42\n",
      "  >>> x = x + 1\n",
      "\n",
      "Use the built-in ``help`` function to view a function's docstring::\n",
      "\n",
      "  >>> help(np.sort)\n",
      "  ... # doctest: +SKIP\n",
      "\n",
      "For some objects, ``np.info(obj)`` may provide additional help.  This is\n",
      "particularly true if you see the line \"Help on ufunc object:\" at the top\n",
      "of the help() page.  Ufuncs are implemented in C, not Python, for speed.\n",
      "The native Python help() does not know how to view their help, but our\n",
      "np.info() function does.\n",
      "\n",
      "Available subpackages\n",
      "---------------------\n",
      "lib\n",
      "    Basic functions used by several sub-packages.\n",
      "random\n",
      "    Core Random Tools\n",
      "linalg\n",
      "    Core Linear Algebra Tools\n",
      "fft\n",
      "    Core FFT routines\n",
      "polynomial\n",
      "    Polynomial tools\n",
      "testing\n",
      "    NumPy testing tools\n",
      "distutils\n",
      "    Enhancements to distutils with support for\n",
      "    Fortran compilers support and more (for Python <= 3.11)\n",
      "\n",
      "Utilities\n",
      "---------\n",
      "test\n",
      "    Run numpy unittests\n",
      "show_config\n",
      "    Show numpy build configuration\n",
      "__version__\n",
      "    NumPy version string\n",
      "\n",
      "Viewing documentation using IPython\n",
      "-----------------------------------\n",
      "\n",
      "Start IPython and import `numpy` usually under the alias ``np``: `import\n",
      "numpy as np`.  Then, directly past or use the ``%cpaste`` magic to paste\n",
      "examples into the shell.  To see which functions are available in `numpy`,\n",
      "type ``np.<TAB>`` (where ``<TAB>`` refers to the TAB key), or use\n",
      "``np.*cos*?<ENTER>`` (where ``<ENTER>`` refers to the ENTER key) to narrow\n",
      "down the list.  To view the docstring for a function, use\n",
      "``np.cos?<ENTER>`` (to view the docstring) and ``np.cos??<ENTER>`` (to view\n",
      "the source code).\n",
      "\n",
      "Copies vs. in-place operation\n",
      "-----------------------------\n",
      "Most of the functions in `numpy` return a copy of the array argument\n",
      "(e.g., `np.sort`).  In-place versions of these functions are often\n",
      "available as array methods, i.e. ``x = np.array([1,2,3]); x.sort()``.\n",
      "Exceptions to this rule are documented."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f975f3",
   "metadata": {},
   "source": [
    "## Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3358b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = \"./datasets/mini_caption\"  \n",
    "CSV_FILE = \"captions.csv\"                       \n",
    "OUT_DIR = \"outputs_task2\"\n",
    "IMAGE_SIZE = 224\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "LR = 1e-4\n",
    "EMBED_DIM = 256\n",
    "TEMPERATURE = 0.07\n",
    "DEVICE = \"cuda\" if __import__('torch').cuda.is_available() else \"cpu\"\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc8feba",
   "metadata": {},
   "source": [
    "## Preparing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageTextDataset(Dataset):\n",
    "    def __init__(self, csv_path: str, data_root: str = \"\", transform=None):\n",
    "        import csv\n",
    "        self.data = []\n",
    "        self.data_root = data_root\n",
    "        with open(csv_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            reader = csv.reader(f)\n",
    "            header = next(reader)\n",
    "            for row in reader:\n",
    "                if len(row) < 2:\n",
    "                    continue\n",
    "                img, cap = row[0].strip(), row[1].strip()\n",
    "                self.data.append((img, cap))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, caption = self.data[idx]\n",
    "        path = os.path.join(self.data_root, img_path) if not os.path.isabs(img_path) else img_path\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, caption, os.path.basename(img_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029231e5",
   "metadata": {},
   "source": [
    "## Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8b9445",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = T.Compose([\n",
    "    T.RandomResizedCrop(IMAGE_SIZE, scale=(0.7,1.0)),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "val_transform = T.Compose([\n",
    "    T.Resize(256),\n",
    "    T.CenterCrop(IMAGE_SIZE),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229,0.224,0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba41091d",
   "metadata": {},
   "source": [
    "## Text encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc65a590",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizer:\n",
    "    def __init__(self, texts: List[str], min_freq: int = 1):\n",
    "        from collections import Counter\n",
    "        toks = []\n",
    "        for t in texts:\n",
    "            toks.extend(self._tokenize(t))\n",
    "        counter = Counter(toks)\n",
    "        self.vocab = {\"<PAD>\":0, \"<UNK>\":1}\n",
    "        for w,c in counter.items():\n",
    "            if c >= min_freq and w not in self.vocab:\n",
    "                self.vocab[w] = len(self.vocab)\n",
    "        self.inv_vocab = {i:w for w,i in self.vocab.items()}\n",
    "\n",
    "    def _tokenize(self, text: str):\n",
    "        return text.lower().split()\n",
    "\n",
    "    def encode(self, text: str, max_len: int = 32):\n",
    "        toks = self._tokenize(text)[:max_len]\n",
    "        ids = [self.vocab.get(t, self.vocab[\"<UNK>\"]) for t in toks]\n",
    "        if len(ids) < max_len:\n",
    "            ids = ids + [self.vocab[\"<PAD>\"]] * (max_len - len(ids))\n",
    "        return ids\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size:int, emb_dim:int=256, max_len:int=32, nhead=4, nlayers=2):\n",
    "        super().__init__()\n",
    "        self.token_embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, max_len, emb_dim))\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=emb_dim, nhead=nhead)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=nlayers)\n",
    "        self.max_len = max_len\n",
    "        self.fc = nn.Linear(emb_dim, EMBED_DIM)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, L) tokens\n",
    "        emb = self.token_embed(x) + self.pos_embed[:, :x.size(1), :]\n",
    "        emb = emb.permute(1,0,2)  # (L,B,E) for transformer\n",
    "        out = self.transformer(emb)  # (L,B,E)\n",
    "        out = out.permute(1,0,2)  # (B,L,E)\n",
    "        # simple pooling: mean excluding PAD\n",
    "        mask = (x != 0).unsqueeze(-1).float()\n",
    "        summed = (out * mask).sum(1)\n",
    "        lengths = mask.sum(1).clamp(min=1.0)\n",
    "        pooled = summed / lengths\n",
    "        out = self.fc(pooled)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27df6a59",
   "metadata": {},
   "source": [
    "## Image encoder with projection head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1eac26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim=EMBED_DIM, pretrained=True):\n",
    "        super().__init__()\n",
    "        backbone = models.resnet50(pretrained=pretrained)\n",
    "        # remove fc\n",
    "        modules = list(backbone.children())[:-1]\n",
    "        self.backbone = nn.Sequential(*modules)  # outputs (B,2048,1,1)\n",
    "        self.fc = nn.Linear(2048, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)  # (B,2048,1,1)\n",
    "        x = x.flatten(1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97efb4f2",
   "metadata": {},
   "source": [
    "## Projection + normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a81dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniCLIP(nn.Module):\n",
    "    def __init__(self, vocab_size, max_len=32):\n",
    "        super().__init__()\n",
    "        self.img_enc = ImageEncoder()\n",
    "        self.txt_enc = TextEncoder(vocab_size, emb_dim=256, max_len=max_len)\n",
    "\n",
    "    def forward(self, images, tokens):\n",
    "        img_feats = self.img_enc(images)  # (B,E)\n",
    "        txt_feats = self.txt_enc(tokens)  # (B,E)\n",
    "        img_norm = F.normalize(img_feats, dim=1)\n",
    "        txt_norm = F.normalize(txt_feats, dim=1)\n",
    "        return img_norm, txt_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ec4758",
   "metadata": {},
   "source": [
    "## Contrastive Loss (NT-Xent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7274dba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(img_emb, txt_emb, temperature=TEMPERATURE):\n",
    "    # img_emb, txt_emb: (B,E) normalized\n",
    "    logits = img_emb @ txt_emb.t() / temperature  # (B,B)\n",
    "    labels = torch.arange(logits.size(0), device=logits.device)\n",
    "    loss_i2t = F.cross_entropy(logits, labels)\n",
    "    loss_t2i = F.cross_entropy(logits.t(), labels)\n",
    "    return (loss_i2t + loss_t2i) / 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a12fa63",
   "metadata": {},
   "source": [
    "## Utilities: collate, training, eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9112a8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def collate_batch(batch, tokenizer, max_len=32, transform=None):\n",
    "    imgs, caps, names = zip(*batch)\n",
    "    imgs = torch.stack(imgs)\n",
    "    tokens = [tokenizer.encode(c, max_len=max_len) for c in caps]\n",
    "    tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "    return imgs, tokens, names\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_embeddings(model, loader, device):\n",
    "    model.eval()\n",
    "    all_img = []\n",
    "    all_txt = []\n",
    "    names = []\n",
    "    for imgs, toks, nms in tqdm(loader, desc=\"Embedding\"):\n",
    "        imgs = imgs.to(device)\n",
    "        toks = toks.to(device)\n",
    "        img_emb, txt_emb = model(imgs, toks)\n",
    "        all_img.append(img_emb.cpu())\n",
    "        all_txt.append(txt_emb.cpu())\n",
    "        names.extend(nms)\n",
    "    all_img = torch.cat(all_img)\n",
    "    all_txt = torch.cat(all_txt)\n",
    "    return all_img.numpy(), all_txt.numpy(), names\n",
    "\n",
    "def retrieval_metrics(img_embs, txt_embs, topk=(1,5,10)):\n",
    "    # compute cosine similarity\n",
    "    sims = img_embs @ txt_embs.T\n",
    "    n = sims.shape[0]\n",
    "    ranks = np.argsort(-sims, axis=1)\n",
    "    recall = {}\n",
    "    for k in topk:\n",
    "        correct = 0\n",
    "        for i in range(n):\n",
    "            # ground truth is at same index i\n",
    "            if i in ranks[i,:k]:\n",
    "                correct += 1\n",
    "        recall[f\"R@{k}\"] = correct / n\n",
    "    # text->image\n",
    "    ranks_t2i = np.argsort(-sims.T, axis=1)\n",
    "    for k in topk:\n",
    "        correct = 0\n",
    "        for i in range(n):\n",
    "            if i in ranks_t2i[i,:k]:\n",
    "                correct += 1\n",
    "        recall[f\"t2i_R@{k}\"] = correct / n\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23365f08",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c862ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, train_loader, val_loader, tokenizer, device, epochs=EPOCHS, lr=LR):\n",
    "    model = model.to(device)\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    best_val = 0.0\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for imgs, toks, _ in tqdm(train_loader, desc=f\"Epoch {epoch}\"):\n",
    "            imgs = imgs.to(device)\n",
    "            toks = toks.to(device)\n",
    "            img_emb, txt_emb = model(imgs, toks)\n",
    "            loss = contrastive_loss(img_emb, txt_emb)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            running_loss += loss.item() * imgs.size(0)\n",
    "        avg_loss = running_loss / len(train_loader.dataset)\n",
    "        # validate\n",
    "        img_embs, txt_embs, _ = compute_embeddings(model, val_loader, device)\n",
    "        metrics = retrieval_metrics(img_embs, txt_embs)\n",
    "        val_r1 = metrics['R@1']\n",
    "        print(f\"Epoch {epoch}: Train Loss {avg_loss:.4f} | Val R@1 {val_r1:.4f}\")\n",
    "        # checkpoint\n",
    "        if val_r1 > best_val:\n",
    "            best_val = val_r1\n",
    "            torch.save({\n",
    "                'model_state': model.state_dict(),\n",
    "                'tokenizer_vocab': tokenizer.vocab\n",
    "            }, os.path.join(OUT_DIR, 'best_mini_clip.pth'))\n",
    "    print(\"Training finished.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2a1d9d",
   "metadata": {},
   "source": [
    "## t-SNE visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132188a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tsne(img_embs, txt_embs, names, save_path=os.path.join(OUT_DIR,'tsne.png')):\n",
    "    feats = np.concatenate([img_embs, txt_embs], axis=0)\n",
    "    labels = ['img']*img_embs.shape[0] + ['txt']*txt_embs.shape[0]\n",
    "    tsne = TSNE(n_components=2, perplexity=30, init='pca', random_state=42)\n",
    "    low = tsne.fit_transform(feats)\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.scatter(low[:img_embs.shape[0],0], low[:img_embs.shape[0],1], label='images', alpha=0.6)\n",
    "    plt.scatter(low[img_embs.shape[0]:,0], low[img_embs.shape[0]:,1], label='texts', alpha=0.6)\n",
    "    plt.legend()\n",
    "    plt.title('t-SNE of image and text embeddings')\n",
    "    plt.savefig(save_path, dpi=200)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02d24ad",
   "metadata": {},
   "source": [
    "## Zero-shot classification using text prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e902fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def zero_shot_classify(model, class_prompts: List[str], dataloader, tokenizer, device):\n",
    "    model.eval()\n",
    "    # encode prompts\n",
    "    token_ids = [tokenizer.encode(p) for p in class_prompts]\n",
    "    token_ids = torch.tensor(token_ids, dtype=torch.long).to(device)\n",
    "    # get prompt embeddings (B_prompts, E)\n",
    "    _, prompt_embs = model(torch.zeros(1,3,IMAGE_SIZE,IMAGE_SIZE).to(device), token_ids) if False else None\n",
    "    # fallback: run text encoder directly\n",
    "    txt_enc = model.txt_enc\n",
    "    prompt_embs = txt_enc(token_ids)\n",
    "    prompt_embs = F.normalize(prompt_embs, dim=1).cpu().numpy()\n",
    "    # compute image embeddings\n",
    "    img_embs, txt_embs, names = compute_embeddings(model, dataloader, device)\n",
    "    sims = img_embs @ prompt_embs.T\n",
    "    preds = sims.argmax(axis=1)\n",
    "    return preds, names"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
