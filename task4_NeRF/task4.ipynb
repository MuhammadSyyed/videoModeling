{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b697493",
   "metadata": {},
   "source": [
    "## Environment setup & installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5998f122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing opencv_python ...\n",
      "Requirement already satisfied: opencv_python in /Users/imranabbas/Documents/videoModeling/.venv/lib/python3.11/site-packages (4.12.0.88)\n",
      "Requirement already satisfied: numpy<2.3.0,>=2 in /Users/imranabbas/Documents/videoModeling/.venv/lib/python3.11/site-packages (from opencv_python) (2.2.6)\n",
      "Installing scikit-image ...\n",
      "Requirement already satisfied: scikit-image in /Users/imranabbas/Documents/videoModeling/.venv/lib/python3.11/site-packages (0.25.2)\n",
      "Requirement already satisfied: numpy>=1.24 in /Users/imranabbas/Documents/videoModeling/.venv/lib/python3.11/site-packages (from scikit-image) (2.2.6)\n",
      "Requirement already satisfied: scipy>=1.11.4 in /Users/imranabbas/Documents/videoModeling/.venv/lib/python3.11/site-packages (from scikit-image) (1.16.3)\n",
      "Requirement already satisfied: networkx>=3.0 in /Users/imranabbas/Documents/videoModeling/.venv/lib/python3.11/site-packages (from scikit-image) (3.5)\n",
      "Requirement already satisfied: pillow>=10.1 in /Users/imranabbas/Documents/videoModeling/.venv/lib/python3.11/site-packages (from scikit-image) (12.0.0)\n",
      "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /Users/imranabbas/Documents/videoModeling/.venv/lib/python3.11/site-packages (from scikit-image) (2.37.2)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /Users/imranabbas/Documents/videoModeling/.venv/lib/python3.11/site-packages (from scikit-image) (2025.10.16)\n",
      "Requirement already satisfied: packaging>=21 in /Users/imranabbas/Documents/videoModeling/.venv/lib/python3.11/site-packages (from scikit-image) (25.0)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in /Users/imranabbas/Documents/videoModeling/.venv/lib/python3.11/site-packages (from scikit-image) (0.4)\n",
      "torch: 2.9.1 numpy: 2.2.6 LPIPS: True\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "import importlib\n",
    "import lpips\n",
    "\n",
    "def ensure(pkg):\n",
    "    try:\n",
    "        importlib.import_module(pkg)\n",
    "    except Exception:\n",
    "        print(f\"Installing {pkg} ...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--no-cache-dir\", pkg])\n",
    "\n",
    "# Core libs\n",
    "ensure(\"torch\")\n",
    "ensure(\"tqdm\")\n",
    "ensure(\"imageio\")\n",
    "ensure(\"numpy\")\n",
    "ensure(\"opencv_python\")  # for image resizing if needed\n",
    "ensure(\"scikit-image\")   # for SSIM\n",
    "# optional LPIPS (may take time); fallback handled later\n",
    "try:\n",
    "    \n",
    "    _LPIPS_AVAILABLE = True\n",
    "except Exception:\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--no-cache-dir\", \"lpips\"])\n",
    "        _LPIPS_AVAILABLE = True\n",
    "    except Exception:\n",
    "        _LPIPS_AVAILABLE = False\n",
    "        print(\"lpips not available — LPIPS metric will be skipped.\")\n",
    "\n",
    "# Confirm versions\n",
    "import torch, numpy as np\n",
    "print(\"torch:\", torch.__version__, \"numpy:\", np.__version__, \"LPIPS:\", _LPIPS_AVAILABLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89999cf",
   "metadata": {},
   "source": [
    "##  Imports, params, positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9cd87918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import math, os, time\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from skimage.metrics import peak_signal_noise_ratio as compute_psnr\n",
    "from skimage.metrics import structural_similarity as compute_ssim\n",
    "import imageio\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "\n",
    "H = 64             \n",
    "W = 64              \n",
    "FOV = 40.0          \n",
    "N_VIEWS = 60        \n",
    "N_VAL = 10        \n",
    "NEAR = 0.2\n",
    "FAR = 2.0\n",
    "N_SAMPLES = 64       \n",
    "POS_ENC_L = 10       \n",
    "DIR_ENC_L = 4        \n",
    "BATCH_RAYS = 4096   \n",
    "LR = 5e-4\n",
    "N_ITERS = 4000\n",
    "\n",
    "\n",
    "def pos_enc(x, L):\n",
    "    # x: (..., D)\n",
    "    enc = [x]\n",
    "    for i in range(L):\n",
    "        for fn in [torch.sin, torch.cos]:\n",
    "            enc.append(fn((2.0**i) * x))\n",
    "    return torch.cat(enc, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b997e5",
   "metadata": {},
   "source": [
    "## Generating synthetic dataset (sphere with Lambert + simple specular)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c21ca1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shapes: (70, 64, 64, 3) (70, 4, 4)\n",
      "Saved sample images in ./nerf_synth\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "def look_at(cam_pos, target=[0,0,0], up=[0,1,0]):\n",
    "    cam_pos = np.array(cam_pos)\n",
    "    target = np.array(target)\n",
    "    up = np.array(up)\n",
    "    z = (cam_pos - target)\n",
    "    z = z / np.linalg.norm(z)\n",
    "    x = np.cross(up, z)\n",
    "    x = x / np.linalg.norm(x)\n",
    "    y = np.cross(z, x)\n",
    "    R = np.stack([x, y, z], axis=1)  \n",
    "    T = -R.T @ cam_pos\n",
    "   \n",
    "    pose = np.eye(4)\n",
    "    pose[:3,:3] = R.T\n",
    "    pose[:3,3] = cam_pos\n",
    "    return pose \n",
    "\n",
    "def render_sphere_image(H, W, cam_pos, f=1.0):\n",
    "    \"\"\"Simple analytic renderer of a sphere centered at origin radius=0.5\"\"\"\n",
    "    yy, xx = np.meshgrid(np.linspace(-1,1,H), np.linspace(-1,1,W), indexing='ij')\n",
    "\n",
    "    dirs = np.stack([xx, -yy, -np.ones_like(xx) * (1.0/f)], axis=-1)\n",
    "    dirs = dirs / (np.linalg.norm(dirs, axis=-1, keepdims=True) + 1e-9)\n",
    "    \n",
    "    cam_to_world = look_at(cam_pos) \n",
    "    R = cam_to_world[:3,:3]\n",
    "    rays_dir_world = dirs.reshape(-1,3) @ R.T\n",
    "    rays_o = np.tile(np.array(cam_pos).reshape(1,3), (rays_dir_world.shape[0],1))\n",
    "\n",
    "\n",
    "    C = np.zeros(3)\n",
    "    r = 0.5\n",
    "    o = rays_o - C\n",
    "    d = rays_dir_world\n",
    "    b = 2.0 * np.sum(o * d, axis=1)\n",
    "    c = np.sum(o*o, axis=1) - r*r\n",
    "    disc = b*b - 4*c\n",
    "    img = np.zeros((H*W,3), dtype=np.float32)\n",
    "    mask = disc > 0\n",
    "    t = np.full(H*W, np.inf)\n",
    "    t[mask] = (-b[mask] - np.sqrt(disc[mask])) / 2.0\n",
    "    hit = t < np.inf\n",
    "    pts = rays_o[hit] + d[hit] * t[hit,None]\n",
    "    normals = (pts - C) / r\n",
    "   \n",
    "    light_dir = np.array([1,1,1]); light_dir = light_dir / np.linalg.norm(light_dir)\n",
    "    lambert = np.clip(np.sum(normals * light_dir, axis=1), 0, 1)\n",
    "   \n",
    "    view_dir = -d[hit] / (np.linalg.norm(d[hit],axis=1,keepdims=True)+1e-9)\n",
    "    halfv = (view_dir + light_dir) / (np.linalg.norm(view_dir + light_dir,axis=1,keepdims=True)+1e-9)\n",
    "    spec = np.clip(np.sum(normals * halfv, axis=1), 0, 1)**50\n",
    "    color = (0.7 * lambert + 0.3 * spec)[:,None] * np.array([[0.8,0.3,0.2]])\n",
    "    img[hit] = color\n",
    "    img = img.reshape(H,W,3)\n",
    "    return np.clip(img,0,1)\n",
    "\n",
    "\n",
    "def gen_dataset(N_views=N_VIEWS, H=H, W=W):\n",
    "    radius = 1.0\n",
    "    all_imgs, all_poses = [], []\n",
    "    angles = np.linspace(0, 2*np.pi, N_views+N_VAL, endpoint=False)\n",
    "    for i, ang in enumerate(angles):\n",
    "        cam_pos = [radius * np.cos(ang), 0.0 + 0.1*np.sin(2*ang), radius * np.sin(ang)]\n",
    "        img = render_sphere_image(H,W, cam_pos, f=1.0)\n",
    "        pose = look_at(cam_pos)\n",
    "        all_imgs.append((img*255).astype(np.uint8))\n",
    "        all_poses.append(pose)\n",
    "    imgs = np.stack(all_imgs)  \n",
    "    poses = np.stack(all_poses)  \n",
    "    return imgs, poses\n",
    "\n",
    "imgs, poses = gen_dataset()\n",
    "print(\"Dataset shapes:\", imgs.shape, poses.shape)\n",
    "\n",
    "os.makedirs(\"nerf_synth\", exist_ok=True)\n",
    "for i in range(3):\n",
    "    imageio.imwrite(f\"nerf_synth/sample_{i}.png\", imgs[i])\n",
    "print(\"Saved sample images in ./nerf_synth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0158d5dc",
   "metadata": {},
   "source": [
    "## Ray Generation and PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54f32a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Focal: 87.91927742254792\n"
     ]
    }
   ],
   "source": [
    "def get_rays(H, W, focal, c2w):\n",
    "\n",
    "    \"\"\"Return rays origins and directions for all pixels in camera c2w (camera-to-world)\"\"\"\n",
    "\n",
    "    i, j = np.meshgrid(np.arange(W), np.arange(H), indexing='xy')\n",
    "    i = i.astype(np.float32); j = j.astype(np.float32)\n",
    "    \n",
    "    dirs = np.stack([(i - W*0.5)/focal, -(j - H*0.5)/focal, -np.ones_like(i)], axis=-1)  # (H,W,3)\n",
    "    \n",
    "    rays_d = dirs.reshape(-1,3) @ c2w[:3,:3].T\n",
    "    rays_o = np.broadcast_to(c2w[:3,3], rays_d.shape)\n",
    "    return rays_o.astype(np.float32), rays_d.astype(np.float32)\n",
    "\n",
    "\n",
    "class RaysDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, imgs, poses, H, W, focal, split=\"train\"):\n",
    "        self.imgs = imgs\n",
    "        self.poses = poses\n",
    "        N = imgs.shape[0]\n",
    "        split_idx = int(N * 0.8)\n",
    "        if split == \"train\":\n",
    "            self.ids = np.arange(0, split_idx)\n",
    "        else:\n",
    "            self.ids = np.arange(split_idx, N)\n",
    "        self.H, self.W, self.focal = H, W, focal\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1000000\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "    \n",
    "        img_id = np.random.choice(self.ids)\n",
    "        img = self.imgs[img_id].astype(np.float32)/255.0\n",
    "        c2w = self.poses[img_id]\n",
    "        rays_o, rays_d = get_rays(self.H, self.W, self.focal, c2w)\n",
    "        inds = np.random.choice(self.H*self.W, size=BATCH_RAYS, replace=False)\n",
    "        rays_o = rays_o[inds]\n",
    "        rays_d = rays_d[inds]\n",
    "        pixels = img.reshape(-1,3)[inds]\n",
    "        return {\n",
    "            'rays_o': torch.from_numpy(rays_o).float(),\n",
    "            'rays_d': torch.from_numpy(rays_d).float(),\n",
    "            'pixels': torch.from_numpy(pixels).float()\n",
    "        }\n",
    "\n",
    "focal = 0.5 * W / math.tan(0.5 * math.radians(FOV))\n",
    "dataset = RaysDataset(imgs, poses, H, W, focal, split=\"train\")\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=1, num_workers=0)\n",
    "print(\"Focal:\", focal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f056c36c",
   "metadata": {},
   "source": [
    "## NeRF MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc5d5464",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNeRF(nn.Module):\n",
    "    def __init__(self, pos_enc_dim=POS_ENC_L, dir_enc_dim=DIR_ENC_L, W_hidden=256):\n",
    "        super().__init__()\n",
    "        in_ch = 3 * (1 + 2*pos_enc_dim)\n",
    "        dir_ch = 3 * (1 + 2*dir_enc_dim)\n",
    "        self.fc1 = nn.Sequential(nn.Linear(in_ch, W_hidden), nn.ReLU(),\n",
    "                                 nn.Linear(W_hidden, W_hidden), nn.ReLU())\n",
    "        self.fc_sigma = nn.Linear(W_hidden, 1)   \n",
    "        self.feature_layer = nn.Linear(W_hidden, W_hidden)\n",
    "        self.color_layer = nn.Sequential(\n",
    "            nn.Linear(W_hidden + dir_ch, W_hidden//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(W_hidden//2, 3),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x_pts, x_dirs):\n",
    "\n",
    "        x_pe = pos_enc(x_pts, POS_ENC_L) \n",
    "        d_pe = pos_enc(x_dirs / (torch.norm(x_dirs, dim=-1, keepdim=True)+1e-9), DIR_ENC_L)\n",
    "        h = self.fc1(x_pe)\n",
    "        sigma = F.relu(self.fc_sigma(h)).squeeze(-1)\n",
    "        feat = self.feature_layer(h)\n",
    "        \n",
    "        color_in = torch.cat([feat, d_pe], dim=-1)\n",
    "        rgb = self.color_layer(color_in)\n",
    "        return rgb, sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d3774c",
   "metadata": {},
   "source": [
    "## Sampling and volume rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "421de70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_along_rays(rays_o, rays_d, n_samples=N_SAMPLES, near=NEAR, far=FAR, perturb=True):\n",
    "    \n",
    "    R = rays_o.shape[0]\n",
    "    t_vals = torch.linspace(0.0, 1.0, steps=n_samples, device=rays_o.device)\n",
    "    z_vals = near * (1 - t_vals) + far * (t_vals)\n",
    "    z_vals = z_vals.expand([R, n_samples]).clone()\n",
    "    if perturb:\n",
    "        mids = 0.5 * (z_vals[:, :-1] + z_vals[:, 1:])\n",
    "        upper = torch.cat([mids, z_vals[:, -1:]], -1)\n",
    "        lower = torch.cat([z_vals[:, :1], mids], -1)\n",
    "        t_rand = torch.rand(z_vals.shape, device=rays_o.device)\n",
    "        z_vals = lower + (upper - lower) * t_rand\n",
    "    pts = rays_o.unsqueeze(1) + rays_d.unsqueeze(1) * z_vals.unsqueeze(2)  # (R, n_samples, 3)\n",
    "    return pts, z_vals\n",
    "\n",
    "def volume_render_radiance_field(model, rays_o, rays_d, n_samples=N_SAMPLES):\n",
    " \n",
    "    device = rays_o.device\n",
    "    pts, z_vals = sample_along_rays(rays_o, rays_d, n_samples=n_samples)\n",
    "    R, S, _ = pts.shape\n",
    "    pts_flat = pts.reshape(-1,3)\n",
    "    dirs = rays_d.unsqueeze(1).expand(R, S, 3).reshape(-1,3)\n",
    "\n",
    "    rgb, sigma = model(pts_flat, dirs)\n",
    "    rgb = rgb.reshape(R, S, 3)\n",
    "    sigma = sigma.reshape(R, S)\n",
    "    \n",
    "    deltas = z_vals[:,1:] - z_vals[:,:-1]\n",
    "    delta_last = 1e10 * torch.ones(R,1, device=device)\n",
    "    deltas = torch.cat([deltas, delta_last], dim=1)  # (R, S)\n",
    "    alpha = 1.0 - torch.exp(-sigma * deltas)\n",
    "    trans = torch.cumprod(torch.cat([torch.ones((R,1), device=device), 1.0 - alpha + 1e-10], dim=1), dim=1)[:, :-1]\n",
    "    weights = alpha * trans  # (R,S)\n",
    "    rgb_map = torch.sum(weights.unsqueeze(-1) * rgb, dim=1)  # (R,3)\n",
    "    depth_map = torch.sum(weights * z_vals, dim=1)\n",
    "    return rgb_map, depth_map, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca67b1d",
   "metadata": {},
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d1748f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleNeRF().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "scaler = None\n",
    "\n",
    "from itertools import islice\n",
    "dataloader = iter(loader)\n",
    "\n",
    "pbar = tqdm(range(N_ITERS), desc=\"train\")\n",
    "loss_log = []\n",
    "for it in pbar:\n",
    "    batch = next(dataloader)\n",
    "    rays_o = batch['rays_o'].squeeze(0).to(device)  # (B,3)\n",
    "    rays_d = batch['rays_d'].squeeze(0).to(device)\n",
    "    pixels = batch['pixels'].squeeze(0).to(device)\n",
    "\n",
    "    rgb_pred, depth_pred, _ = volume_render_radiance_field(model, rays_o, rays_d, n_samples=N_SAMPLES)\n",
    "    loss = F.mse_loss(rgb_pred, pixels)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    loss_log.append(loss.item())\n",
    "\n",
    "    if it % 200 == 0 or it==N_ITERS-1:\n",
    "        \n",
    "        pbar.set_postfix({'it': it, 'loss': float(loss.item())})\n",
    "\n",
    "        torch.save(model.state_dict(), \"nerf_checkpoint.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f40a3b",
   "metadata": {},
   "source": [
    "## Rendering holdout views & saving as gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fdf2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_image(model, c2w, H=H, W=W, focal=focal, nsamples=N_SAMPLES):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        rays_o, rays_d = get_rays(H, W, focal, c2w)\n",
    "        rays_o = torch.from_numpy(rays_o).float().to(device)\n",
    "        rays_d = torch.from_numpy(rays_d).float().to(device)\n",
    "       \n",
    "        chunk = 8192\n",
    "        rgb_chunks = []\n",
    "        for i in range(0, rays_o.shape[0], chunk):\n",
    "            ro = rays_o[i:i+chunk]\n",
    "            rd = rays_d[i:i+chunk]\n",
    "            rgb, depth, _ = volume_render_radiance_field(model, ro, rd, n_samples=nsamples)\n",
    "            rgb_chunks.append(rgb.cpu().numpy())\n",
    "        rgb = np.concatenate(rgb_chunks, axis=0)\n",
    "        img = rgb.reshape(H, W, 3)\n",
    "        return np.clip(img,0,1)\n",
    "\n",
    "\n",
    "N = imgs.shape[0]\n",
    "split_idx = int(N*0.8)\n",
    "val_ids = np.arange(split_idx, N)\n",
    "os.makedirs(\"nerf_renders\", exist_ok=True)\n",
    "frames = []\n",
    "for vid in val_ids:\n",
    "    c2w = poses[vid]\n",
    "    img = render_image(model, c2w)\n",
    "    gt = imgs[vid].astype(np.float32)/255.0\n",
    "   \n",
    "    combined = (np.concatenate([gt, img], axis=1)*255).astype(np.uint8)\n",
    "    imageio.imwrite(f\"nerf_renders/val_{vid}.png\", combined)\n",
    "    frames.append((img*255).astype(np.uint8))\n",
    "\n",
    "print(\"Saved renders to nerf_renders/ .\")\n",
    "\n",
    "imageio.mimsave(\"nerf_renders/novel_views.gif\", frames, fps=4)\n",
    "print(\"Saved gif: nerf_renders/novel_views.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b8f3ac",
   "metadata": {},
   "source": [
    "## Computing metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4fa89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import img_as_float32\n",
    "psnrs, ssims, lpips_vals = [], [], []\n",
    "lpips_model = None\n",
    "if _LPIPS_AVAILABLE:\n",
    "    import lpips\n",
    "    lpips_model = lpips.LPIPS(net='vgg').to(device)\n",
    "\n",
    "for vid in val_ids:\n",
    "    rendered = render_image(model, poses[vid])\n",
    "    gt = imgs[vid].astype(np.float32)/255.0\n",
    "    psnrs.append(compute_psnr(gt, rendered, data_range=1.0))\n",
    "\n",
    "    ssim_val = compute_ssim((gt*255).astype(np.uint8), (rendered*255).astype(np.uint8), multichannel=True, data_range=255)\n",
    "    ssims.append(ssim_val)\n",
    "    if lpips_model is not None:\n",
    "\n",
    "        # LPIPS expects tensors [-1,1]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            a = torch.from_numpy((gt*2-1).transpose(2,0,1)).unsqueeze(0).float().to(device)\n",
    "            b = torch.from_numpy((rendered*2-1).transpose(2,0,1)).unsqueeze(0).float().to(device)\n",
    "            lp = lpips_model(a, b).item()\n",
    "        lpips_vals.append(lp)\n",
    "\n",
    "print(\"PSNR:\", np.mean(psnrs), \"SSIM:\", np.mean(ssims))\n",
    "if lpips_model is not None:\n",
    "    print(\"LPIPS:\", np.mean(lpips_vals))\n",
    "else:\n",
    "    print(\"LPIPS skipped (lpips package unavailable).\")\n",
    "\n",
    "metrics = {\"psnr_mean\": float(np.mean(psnrs)), \"ssim_mean\": float(np.mean(ssims)), \"lpips_mean\": float(np.mean(lpips_vals)) if lpips_vals else None}\n",
    "import json\n",
    "with open(\"nerf_renders/metrics.json\",\"w\") as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "print(\"Saved metrics to nerf_renders/metrics.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5057d8",
   "metadata": {},
   "source": [
    "## Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be82709",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"nerf_model_final.pth\")\n",
    "print(\"Saved model: nerf_model_final.pth\")\n",
    "\n",
    "print(\"GIF saved at nerf_renders/novel_views.gif (use ffmpeg to convert to mp4 if desired)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420e1a95",
   "metadata": {},
   "source": [
    "## Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58531ee0",
   "metadata": {},
   "source": [
    "Neural Radiance Fields (NeRF) provide a compact, differentiable representation of 3D appearance by mapping\n",
    "continuous 3D coordinates and viewing directions to emitted color and volumetric density using a multi-layer\n",
    "perceptron (MLP). This approach implicitly encodes geometry—rather than storing explicit surfaces—so geometry\n",
    "emerges from volumetric density predicted across sampled points along camera rays.\n",
    "\n",
    "This simplified implementation builds an MLP that receives positional encoded 3D coordinates and view\n",
    "direction encodings. Positional encoding maps low-dimensional inputs into a higher-frequency space using\n",
    "sine/cosine bases, enabling the network to represent high-frequency variation (sharp edges, fine geometry)\n",
    "despite using smooth activation functions. The MLP uses a density head (sigma) and a color head; the density\n",
    "controls opacity along sampled ray segments while the color head conditions on both spatial features and\n",
    "view direction to reproduce view-dependent effects (specular highlights).\n",
    "\n",
    "Volume rendering integrates density-weighted colors along a ray: points are sampled between near and far\n",
    "bounds; network outputs color and density; weights are computed from density and distances between samples;\n",
    "and the weighted sum yields the pixel color and depth estimate. Training supervises rendered colors against\n",
    "ground-truth images for many camera poses. The optimization drives the MLP to assign high density where\n",
    "rays consistently intersect an object's surface thus implicitly reconstructing the geometry, while view-dependent\n",
    "color allows realistic appearance variations.\n",
    "\n",
    "Trade-offs: sample count, model width, and training views govern the speed-vs-quality trade-off. More samples\n",
    "and larger networks improve fidelity but increase compute and memory. Positional encoding frequency boosts\n",
    "detail but can lead to sneaking high-frequency artifacts or slower convergence. Practical speedups include\n",
    "coarse-to-fine sampling strategies, hierarchical sampling, smaller inference resolutions, caching learned\n",
    "features, or using smaller but efficient architectures (e.g., instant-ngp hash encodings or voxel-grid\n",
    "priors) for real-time rendering. For quality, increasing training views and iterations yields better geometry\n",
    "and view synthesis.\n",
    "\n",
    "In this assignment, we used a synthetic sphere dataset to validate the whole pipeline quickly. Despite the\n",
    "dataset's simplicity compared with complex natural scenes, the model demonstrates how continuous volumetric\n",
    "representations capture both geometry (density) and appearance (view-conditioned color). The provided code\n",
    "is modular — replace the synthetic dataset with Blender renders and poses for real scenes. Quantitative metrics\n",
    "(PSNR/SSIM and LPIPS if available) and saved renders are provided to evaluate and compare different speed/\n",
    "quality configurations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168ddde8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
