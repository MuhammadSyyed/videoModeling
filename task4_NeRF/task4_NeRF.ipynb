{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b697493",
   "metadata": {},
   "source": [
    "## Environment setup & installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb552c92-b54e-44a8-a981-70929d0769c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install numpy==1.26.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4713ce1",
   "metadata": {},
   "source": [
    "## Imports and helper installer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f4c51c-7e4f-40aa-9630-cc6506d14401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7086849bd530>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys, subprocess, importlib, math, time\n",
    "from types import SimpleNamespace\n",
    "\n",
    "def try_import(name, pypi_name=None):\n",
    "    try:\n",
    "        return importlib.import_module(name)\n",
    "    except Exception:\n",
    "        if pypi_name is None:\n",
    "            pypi_name = name\n",
    "        print(f\"Trying to pip install {pypi_name} ...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--no-cache-dir\", pypi_name])\n",
    "        return importlib.import_module(name)\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torchvision = None\n",
    "imageio = None\n",
    "try:\n",
    "    torchvision = importlib.import_module(\"torchvision\")\n",
    "except Exception:\n",
    "    try:\n",
    "        torchvision = try_import(\"torchvision\")\n",
    "    except Exception:\n",
    "        print(\"torchvision not available; will use torch-only fallback for saving.\")\n",
    "\n",
    "try:\n",
    "    imageio = importlib.import_module(\"imageio\")\n",
    "except Exception:\n",
    "    try:\n",
    "        imageio = try_import(\"imageio\")\n",
    "    except Exception:\n",
    "        print(\"imageio not available; fallback to saving tensors as .pt\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88afb12",
   "metadata": {},
   "source": [
    "## Positional encoding (like original NeRF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ebf67b-9f68-4420-8338-a33725b3dbc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PE out shape: torch.Size([2, 36])\n"
     ]
    }
   ],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, in_dims, num_freqs=10, log_scale=True):\n",
    "        super().__init__()\n",
    "        self.in_dims = in_dims\n",
    "        self.num_freqs = num_freqs\n",
    "        self.log_scale = log_scale\n",
    "        if log_scale:\n",
    "            self.freq_bands = 2.0 ** torch.arange(num_freqs).float()\n",
    "        else:\n",
    "            self.freq_bands = torch.linspace(1.0, 2.0 ** (num_freqs-1), num_freqs)\n",
    "        self.output_dims = in_dims * num_freqs * 2\n",
    "\n",
    "    def forward(self, x):\n",
    "       \n",
    "        out = []\n",
    "        for freq in self.freq_bands.to(x.device):\n",
    "            out.append(torch.sin(x * freq))\n",
    "            out.append(torch.cos(x * freq))\n",
    "        return torch.cat(out, dim=-1)\n",
    "\n",
    "pe = PositionalEncoding(3, num_freqs=6)\n",
    "t = torch.randn(2,3)\n",
    "print(\"PE out shape:\", pe(t).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabe45a9",
   "metadata": {},
   "source": [
    "## NeRF MLP (small, fast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e0af1a-b3b9-4dcc-a329-baf0971c1712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model param count: 132868\n"
     ]
    }
   ],
   "source": [
    "class NeRFSmall(nn.Module):\n",
    "    def __init__(self, pos_dim, dir_dim, d_hidden=128, n_layers=6, skips=[3]):\n",
    "        super().__init__()\n",
    "        self.skips = skips\n",
    "        self.layers = nn.ModuleList()\n",
    "        in_dim = pos_dim\n",
    "        for i in range(n_layers):\n",
    "            self.layers.append(nn.Linear(in_dim, d_hidden))\n",
    "            in_dim = d_hidden\n",
    "            if i in self.skips:\n",
    "                in_dim += pos_dim\n",
    "        # sigma head\n",
    "        self.sigma_head = nn.Sequential(nn.Linear(d_hidden, d_hidden//2), nn.ReLU(), nn.Linear(d_hidden//2,1))\n",
    "        # feature to RGB\n",
    "        self.feature_layer = nn.Linear(d_hidden, d_hidden)\n",
    "        self.dir_fc = nn.Sequential(nn.Linear(dir_dim + d_hidden, d_hidden//2), nn.ReLU(), nn.Linear(d_hidden//2, 3))\n",
    "\n",
    "    def forward(self, x_pos_enc, x_dir_enc):\n",
    "        x = x_pos_enc\n",
    "        h = x\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            h = F.relu(layer(h))\n",
    "            if i in self.skips:\n",
    "                h = torch.cat([h, x_pos_enc], dim=-1)\n",
    "        sigma = F.relu(self.sigma_head(h)).squeeze(-1)  # (N,)\n",
    "        feat = F.relu(self.feature_layer(h))\n",
    "        # concat feat and dir encoding\n",
    "        h_dir = torch.cat([feat, x_dir_enc], dim=-1)\n",
    "        rgb = torch.sigmoid(self.dir_fc(h_dir))  # (N,3)\n",
    "        return rgb, sigma\n",
    "\n",
    "pe_pos = PositionalEncoding(3, num_freqs=10)   # yields 3*10*2 = 60\n",
    "pe_dir = PositionalEncoding(3, num_freqs=4)    # yields 3*4*2 = 24\n",
    "model = NeRFSmall(pos_dim=pe_pos.output_dims, dir_dim=pe_dir.output_dims).to(device)\n",
    "print(\"Model param count:\", sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1011ce",
   "metadata": {},
   "source": [
    "## Rays, sample points, and volume rendering utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4431b347-006e-4a2b-9b6b-40dbca719c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rays_from_cam(origin, lookat, up, fov_deg, H, W):\n",
    "    # origin, lookat, up: tensors (3,)\n",
    "    # returns rays_o (H*W,3), rays_d (H*W,3)\n",
    "    z = (lookat - origin); z = z / z.norm()\n",
    "    x = torch.cross(z, up); x = x / x.norm()\n",
    "    y = torch.cross(x, z); y = y / y.norm()\n",
    "    # camera space sampling\n",
    "    i = torch.linspace(0, W-1, W, device=origin.device)\n",
    "    j = torch.linspace(0, H-1, H, device=origin.device)\n",
    "    jj, ii = torch.meshgrid(j, i, indexing='ij')\n",
    "    # normalized device coords\n",
    "    aspect = W/H\n",
    "    fov = math.radians(fov_deg)\n",
    "    px = ( (ii + 0.5)/W - 0.5) * 2 * math.tan(fov/2) * aspect\n",
    "    py = ( (jj + 0.5)/H - 0.5) * 2 * math.tan(fov/2)\n",
    "    dirs = (px.unsqueeze(-1)*x + py.unsqueeze(-1)*y + z.unsqueeze(0).unsqueeze(0))\n",
    "    dirs = dirs / torch.norm(dirs, dim=-1, keepdim=True)\n",
    "    rays_o = origin.expand(H, W, 3).reshape(-1,3)\n",
    "    rays_d = dirs.reshape(-1,3)\n",
    "    return rays_o, rays_d\n",
    "\n",
    "def sample_along_rays(rays_o, rays_d, near, far, n_samples, perturb=True):\n",
    "    # rays_o,d: (N,3)\n",
    "    N = rays_o.shape[0]\n",
    "    t_vals = torch.linspace(0.0, 1.0, steps=n_samples, device=rays_o.device).unsqueeze(0).expand(N, -1)\n",
    "    z_vals = near * (1 - t_vals) + far * t_vals  # linear depth\n",
    "    if perturb:\n",
    "        mids = 0.5 * (z_vals[:, :-1] + z_vals[:, 1:])\n",
    "        upper = torch.cat([mids, z_vals[:, -1:]], -1)\n",
    "        lower = torch.cat([z_vals[:, :1], mids], -1)\n",
    "        t_rand = torch.rand(z_vals.shape, device=rays_o.device)\n",
    "        z_vals = lower + (upper - lower) * t_rand\n",
    "    pts = rays_o.unsqueeze(1) + rays_d.unsqueeze(1) * z_vals.unsqueeze(-1)\n",
    "    return pts, z_vals\n",
    "\n",
    "def volume_render_rgb(model, pts, dirs, z_vals, pe_pos, pe_dir):\n",
    "    # pts: (N, S, 3); dirs: (N,3); z_vals: (N,S)\n",
    "    N, S, _ = pts.shape\n",
    "    pts_flat = pts.reshape(-1,3)\n",
    "    dirs_exp = dirs.unsqueeze(1).expand(-1,S,-1).reshape(-1,3)\n",
    "    pos_enc = pe_pos(pts_flat)\n",
    "    dir_enc = pe_dir(dirs_exp)\n",
    "    rgb, sigma = model(pos_enc, dir_enc)  # both (N*S, ...)\n",
    "    rgb = rgb.reshape(N, S, 3)\n",
    "    sigma = sigma.reshape(N, S)\n",
    "    del pts_flat, dirs_exp, pos_enc, dir_enc\n",
    "    dists = z_vals[...,1:] - z_vals[...,:-1]\n",
    "    dists = torch.cat([dists, 1e10*torch.ones_like(dists[..., :1])], -1)  # (N,S)\n",
    "    alpha = 1.0 - torch.exp(-sigma * dists)\n",
    "    trans = torch.cumprod(torch.cat([torch.ones((N,1), device=alpha.device), 1.0 - alpha + 1e-10], -1), -1)[:, :-1]\n",
    "    weights = alpha * trans  # (N,S)\n",
    "    rgb_map = (weights.unsqueeze(-1) * rgb).sum(dim=1)  # (N,3)\n",
    "    depth_map = (weights * z_vals).sum(dim=1)\n",
    "    acc_map = weights.sum(dim=1)\n",
    "    return rgb_map, depth_map, acc_map, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733cbc42-e7a9-419f-9dd5-d0bb6b9364d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GT imgs rendered: 4 torch.Size([4096, 3])\n"
     ]
    }
   ],
   "source": [
    "def gt_density_color(pts):\n",
    "    c1 = torch.tensor([0.0, 0.0, 0.6], device=pts.device)\n",
    "    c2 = torch.tensor([0.2, -0.2, 0.3], device=pts.device)\n",
    "    s1 = 0.25\n",
    "    s2 = 0.18\n",
    "    d1 = torch.exp(-((pts - c1)**2).sum(-1)/(2*s1**2))\n",
    "    d2 = torch.exp(-((pts - c2)**2).sum(-1)/(2*s2**2))\n",
    "    sigma = 10.0 * (d1 + 0.7*d2)\n",
    "    # color field: vary with position\n",
    "    rgb = torch.stack([0.5 + 0.5*torch.tanh(pts[:,0]*3.0),\n",
    "                       0.6 + 0.4*torch.tanh(pts[:,1]*3.0),\n",
    "                       0.4 + 0.6*torch.tanh(pts[:,2]*3.0)], -1)\n",
    "    # modulate color by relative blob strength\n",
    "    blob = (d1.unsqueeze(-1) * torch.tensor([1.0,0.7,0.5], device=pts.device) +\n",
    "            d2.unsqueeze(-1) * torch.tensor([0.4,0.8,1.0], device=pts.device))\n",
    "    rgb = rgb * (0.4 + 0.8*blob)\n",
    "    return rgb, sigma\n",
    "\n",
    "def render_gt_images(origins, lookats, ups, fov, H, W, near, far, n_samples, pe_pos, pe_dir):\n",
    "    # returns list of (rgb_images: torch (H*W,3) clipped [0,1])\n",
    "    imgs = []\n",
    "    depths = []\n",
    "    for o, la, up in zip(origins, lookats, ups):\n",
    "        rays_o, rays_d = get_rays_from_cam(o, la, up, fov, H, W)\n",
    "        pts, z_vals = sample_along_rays(rays_o, rays_d, near, far, n_samples, perturb=False)\n",
    "        N, S, _ = pts.shape\n",
    "        pts_flat = pts.reshape(-1,3)\n",
    "        rgb_field, sigma_field = gt_density_color(pts_flat)\n",
    "        rgb_field = rgb_field.reshape(N, S, 3)\n",
    "        sigma_field = sigma_field.reshape(N, S)\n",
    "        # do volume rendering (same formula)\n",
    "        dists = z_vals[...,1:] - z_vals[...,:-1]\n",
    "        dists = torch.cat([dists, 1e10*torch.ones_like(dists[..., :1])], -1)\n",
    "        alpha = 1.0 - torch.exp(-sigma_field * dists)\n",
    "        trans = torch.cumprod(torch.cat([torch.ones((N,1), device=alpha.device), 1.0 - alpha + 1e-10], -1), -1)[:, :-1]\n",
    "        weights = alpha * trans\n",
    "        rgb_map = (weights.unsqueeze(-1) * rgb_field).sum(dim=1)\n",
    "        depth_map = (weights * z_vals).sum(dim=1)\n",
    "        imgs.append(rgb_map)\n",
    "        depths.append(depth_map)\n",
    "    return imgs, depths\n",
    "\n",
    "# Quick generate 64 training views around scene\n",
    "def generate_cameras(n_views=64, radius=1.2, device=device):\n",
    "    origins = []\n",
    "    lookats = []\n",
    "    ups = []\n",
    "    for i in range(n_views):\n",
    "        theta = 2*math.pi * i / n_views\n",
    "        phi = math.radians(10)  # slight elevation\n",
    "        x = radius * math.cos(theta) * math.cos(phi)\n",
    "        y = radius * math.sin(theta) * math.cos(phi)\n",
    "        z = radius * math.sin(phi)\n",
    "        origin = torch.tensor([x,y,z], device=device)\n",
    "        lookat = torch.tensor([0.0,0.0,0.3], device=device)\n",
    "        up = torch.tensor([0.0,0.0,1.0], device=device)\n",
    "        origins.append(origin)\n",
    "        lookats.append(lookat)\n",
    "        ups.append(up)\n",
    "    return origins, lookats, ups\n",
    "\n",
    "# test GT render of 4 views (low res)\n",
    "H, W = 64, 64\n",
    "origins, lookats, ups = generate_cameras(8)\n",
    "imgs, depths = render_gt_images(origins[:4], lookats[:4], ups[:4], fov=40, H=H, W=W, near=0.1, far=2.0, n_samples=64, pe_pos=pe, pe_dir=pe)\n",
    "print(\"GT imgs rendered:\", len(imgs), imgs[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5924ae3a",
   "metadata": {},
   "source": [
    "## Dataset returning ray bundles for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17062dc4-6c23-426f-bc44-de9626b468cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 16384\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class RaysDataset(Dataset):\n",
    "    def __init__(self, imgs, depths, origins, lookats, ups, H, W, fov, near, far, n_samples):\n",
    "        # imgs: list of (H*W,3) tensors already rendered\n",
    "        self.H = H; self.W = W; self.fov = fov\n",
    "        self.near = near; self.far = far; self.n_samples = n_samples\n",
    "        self.origins = origins; self.lookats = lookats; self.ups = ups\n",
    "        self.imgs = imgs  # list of tensors (H*W,3)\n",
    "        self.views = len(imgs)\n",
    "        # create ray sets cached per view\n",
    "        self.rays_o = []\n",
    "        self.rays_d = []\n",
    "        for o, la, up in zip(origins, lookats, ups):\n",
    "            ro, rd = get_rays_from_cam(o, la, up, fov, H, W)\n",
    "            self.rays_o.append(ro)\n",
    "            self.rays_d.append(rd)\n",
    "        # flatten all views into one big list\n",
    "        self.view_indices = []\n",
    "        for v in range(self.views):\n",
    "            self.view_indices += [v] * (H*W)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.view_indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        v = self.view_indices[idx]\n",
    "        pix_idx = idx % (self.H * self.W)\n",
    "        ro = self.rays_o[v][pix_idx]\n",
    "        rd = self.rays_d[v][pix_idx]\n",
    "        pixel = self.imgs[v][pix_idx]\n",
    "        return {'rays_o': ro, 'rays_d': rd, 'pixel': pixel}\n",
    "\n",
    "# collate to batch rays\n",
    "def collate_rays(batch):\n",
    "    rays_o = torch.stack([b['rays_o'] for b in batch], dim=0)\n",
    "    rays_d = torch.stack([b['rays_d'] for b in batch], dim=0)\n",
    "    pixels = torch.stack([b['pixel'] for b in batch], dim=0)\n",
    "    return {'rays_o': rays_o, 'rays_d': rays_d, 'pixels': pixels}\n",
    "\n",
    "# Build dataset from previously rendered GT images\n",
    "train_views = len(imgs)  # from previous GT generation; you can generate more (e.g., 64)\n",
    "dataset = RaysDataset(imgs, depths, origins[:train_views], lookats[:train_views], ups[:train_views], H=H, W=W, fov=40, near=0.1, far=2.0, n_samples=64)\n",
    "dataloader = DataLoader(dataset, batch_size=1024, shuffle=True, collate_fn=collate_rays, num_workers=0)\n",
    "print(\"Dataset size:\", len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f106a54",
   "metadata": {},
   "source": [
    "## Precompute rays for ALL training views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8436d527-bc85-4bdd-853b-756d4081eb3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precomputed rays for: 8 views.\n",
      "Each rays tensor shape: torch.Size([4096, 3]) torch.Size([4096, 3])\n"
     ]
    }
   ],
   "source": [
    "rays_o_all = []\n",
    "rays_d_all = []\n",
    "\n",
    "for o, la, up in zip(origins, lookats, ups):\n",
    "    ro, rd = get_rays_from_cam(o, la, up, 40, H, W)\n",
    "    rays_o_all.append(ro.to(device))  # store as torch tensors\n",
    "    rays_d_all.append(rd.to(device))\n",
    "\n",
    "print(\"Precomputed rays for:\", len(rays_o_all), \"views.\")\n",
    "print(\"Each rays tensor shape:\", rays_o_all[0].shape, rays_d_all[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc195e1",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e8ea56-cf36-495f-ba27-08c5cdeb27bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter 200] Loss = 0.005196\n",
      "[Iter 400] Loss = 0.000717\n",
      "[Iter 600] Loss = 0.000394\n",
      "[Iter 800] Loss = 0.001225\n",
      "[Iter 1000] Loss = 0.000163\n",
      "[Iter 1200] Loss = 0.000101\n",
      "[Iter 1400] Loss = 0.000164\n",
      "[Iter 1600] Loss = 0.000095\n",
      "[Iter 1800] Loss = 0.000053\n",
      "[Iter 2000] Loss = 0.000069\n",
      "[Iter 2200] Loss = 0.000248\n",
      "[Iter 2400] Loss = 0.000341\n",
      "[Iter 2600] Loss = 0.000101\n",
      "[Iter 2800] Loss = 0.000025\n",
      "[Iter 3000] Loss = 0.000033\n",
      "[Iter 3200] Loss = 0.000991\n",
      "[Iter 3400] Loss = 0.000020\n",
      "[Iter 3600] Loss = 0.000016\n",
      "[Iter 3800] Loss = 0.000013\n",
      "[Iter 4000] Loss = 0.000009\n",
      "Training completed. Saved to nerf_small.pth.\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "model.train()\n",
    "loss_log = []\n",
    "\n",
    "num_views = len(imgs)  \n",
    "\n",
    "for it in range(1, 4001):   # 4000 iterations\n",
    "\n",
    "    view_idx = torch.randint(low=0, high=num_views, size=(1,)).item()\n",
    "\n",
    " \n",
    "    gt_img = imgs[view_idx].to(device)          # (H*W, 3)\n",
    "    ro = rays_o_all[view_idx].to(device)        # rays_o_all: precomputed rays\n",
    "    rd = rays_d_all[view_idx].to(device)\n",
    "\n",
    "   \n",
    "    idx = torch.randint(low=0, high=H*W, size=(1024,), device=device)\n",
    "\n",
    "    rays_o = ro[idx]        # (1024,3)\n",
    "    rays_d = rd[idx]        # (1024,3)\n",
    "    target = gt_img[idx]    # (1024,3)\n",
    "\n",
    "  \n",
    "    pts, z_vals = sample_along_rays(rays_o, rays_d, near=0.1, far=2.0, n_samples=64, perturb=True)\n",
    "    rgb_pred, depth_pred, acc, weights = volume_render_rgb(model, pts, rays_d, z_vals, pe_pos, pe_dir)\n",
    "\n",
    "    loss = criterion(rgb_pred, target)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    loss_log.append(loss.item())\n",
    "\n",
    "    if it % 200 == 0:\n",
    "        print(f\"[Iter {it}] Loss = {loss.item():.6f}\")\n",
    "\n",
    "torch.save(model.state_dict(), \"outputs_task4/nerf_small.pth\")\n",
    "print(\"Training completed. Saved to nerf_small.pth.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd80e685",
   "metadata": {},
   "source": [
    "## Rendering novel views and compute PSNR/SSIM and a LPIPS-like perceptual distance (VGG features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dd5bd1-0be5-48f3-9339-e2c31f31487d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PSNR: 49.135 dB, SSIM: 0.9967, LPIPS-like: 0.022604\n"
     ]
    }
   ],
   "source": [
    "def render_novel_view(model, origin, lookat, up, fov, H, W, near, far, n_samples, pe_pos, pe_dir, chunk=4096):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        rays_o, rays_d = get_rays_from_cam(origin, lookat, up, fov, H, W)\n",
    "        N = rays_o.shape[0]\n",
    "        all_rgb = []\n",
    "        for i in range(0, N, chunk):\n",
    "            ro = rays_o[i:i+chunk].to(device)\n",
    "            rd = rays_d[i:i+chunk].to(device)\n",
    "            pts, z_vals = sample_along_rays(ro, rd, near, far, n_samples, perturb=False)\n",
    "            rgb_map, depth_map, acc, weights = volume_render_rgb(model, pts, rd, z_vals, pe_pos, pe_dir)\n",
    "            all_rgb.append(rgb_map)\n",
    "        rgb = torch.cat(all_rgb, dim=0)\n",
    "    return rgb  # (H*W,3)\n",
    "\n",
    "\n",
    "def psnr_torch(img1, img2, max_val=1.0):\n",
    "    mse = torch.mean((img1 - img2)**2)\n",
    "    return -10.0 * torch.log10(mse + 1e-12)\n",
    "\n",
    "\n",
    "def ssim_torch(img1, img2, C1=0.01**2, C2=0.03**2, win_size=7):\n",
    "    # img: (N,3) flatten. We'll reshape to (1,1,H,W) for each channel and average SSIM.\n",
    "    def _ssim_channel(a,b):\n",
    "        # a,b: (1,1,H,W)\n",
    "        mu1 = F.avg_pool2d(a, kernel_size=win_size, stride=1, padding=win_size//2)\n",
    "        mu2 = F.avg_pool2d(b, kernel_size=win_size, stride=1, padding=win_size//2)\n",
    "        mu1_sq = mu1*mu1\n",
    "        mu2_sq = mu2*mu2\n",
    "        mu1_mu2 = mu1*mu2\n",
    "        sigma1_sq = F.avg_pool2d(a*a, win_size, stride=1, padding=win_size//2) - mu1_sq\n",
    "        sigma2_sq = F.avg_pool2d(b*b, win_size, stride=1, padding=win_size//2) - mu2_sq\n",
    "        sigma12 = F.avg_pool2d(a*b, win_size, stride=1, padding=win_size//2) - mu1_mu2\n",
    "        ssim_map = ((2*mu1_mu2 + C1)*(2*sigma12 + C2))/((mu1_sq + mu2_sq + C1)*(sigma1_sq + sigma2_sq + C2) + 1e-12)\n",
    "        return ssim_map.mean()\n",
    "    Hc = int(math.sqrt(img1.shape[0]))\n",
    "    a = img1.reshape(Hc, Hc, 3).permute(2,0,1).unsqueeze(1)  # (3,1,H,W)\n",
    "    b = img2.reshape(Hc, Hc, 3).permute(2,0,1).unsqueeze(1)\n",
    "    ssim_vals = []\n",
    "    for ch in range(3):\n",
    "        ssim_vals.append(_ssim_channel(a[ch:ch+1], b[ch:ch+1]).item())\n",
    "    return sum(ssim_vals)/3.0\n",
    "\n",
    "# LPIPS-like — use VGG feature L2 distance normalized (fallback if torchvision unavailable)\n",
    "vgg = None\n",
    "if torchvision is not None:\n",
    "    try:\n",
    "        vgg = torchvision.models.vgg16(pretrained=True).features.eval().to(device)\n",
    "        for p in vgg.parameters(): p.requires_grad = False\n",
    "    except Exception as e:\n",
    "        print(\"Could not load vgg16:\", e)\n",
    "        vgg = None\n",
    "\n",
    "def lpips_like(img1, img2):\n",
    "    # img1,img2: (N,3) flatten [0,1]. Convert to NCHW and pass through VGG layers and compute L2 of features\n",
    "    if vgg is None:\n",
    "        # fallback: simple L2 normalized distance\n",
    "        return torch.mean((img1 - img2).pow(2)).item()\n",
    "    Hc = int(math.sqrt(img1.shape[0]))\n",
    "    a = img1.reshape(Hc, Hc, 3).permute(2,0,1).unsqueeze(0)  # 1,3,H,W\n",
    "    b = img2.reshape(Hc, Hc, 3).permute(2,0,1).unsqueeze(0)\n",
    "    # normalize to ImageNet mean/std\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406], device=device).view(1,3,1,1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225], device=device).view(1,3,1,1)\n",
    "    a = (a.to(device) - mean) / std\n",
    "    b = (b.to(device) - mean) / std\n",
    "    feats_a = vgg(a)\n",
    "    feats_b = vgg(b)\n",
    "    return torch.mean((feats_a - feats_b).pow(2)).item()\n",
    "\n",
    "# Render a novel held-out view and compute metrics vs GT (we have analytic GT)\n",
    "novel_idx = 0\n",
    "# prepare more GT views for training above; here we re-use GT for eval (but normally you'd hold out)\n",
    "eval_origin = origins[0]; eval_lookat = lookats[0]; eval_up = ups[0]\n",
    "rendered = render_novel_view(model, eval_origin, eval_lookat, eval_up, fov=40, H=H, W=W, near=0.1, far=2.0, n_samples=128, pe_pos=pe_pos, pe_dir=pe_dir)\n",
    "gt = imgs[0].to(device)  # from GT earlier\n",
    "psnr_val = psnr_torch(rendered, gt)\n",
    "ssim_val = ssim_torch(rendered.cpu(), gt.cpu())\n",
    "lpips_val = lpips_like(rendered, gt)\n",
    "print(f\"PSNR: {psnr_val.item():.3f} dB, SSIM: {ssim_val:.4f}, LPIPS-like: {lpips_val:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbf206b",
   "metadata": {},
   "source": [
    "## Save novel view frames as PNG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52041ae8-b801-4784-a719-e5031fc1badb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: novel_views_png/view_000.png\n",
      "Saved: novel_views_png/view_001.png\n",
      "Saved: novel_views_png/view_002.png\n",
      "Saved: novel_views_png/view_003.png\n",
      "Saved: novel_views_png/view_004.png\n",
      "Saved: novel_views_png/view_005.png\n",
      "Saved: novel_views_png/view_006.png\n",
      "Saved: novel_views_png/view_007.png\n",
      "Saved: novel_views_png/view_008.png\n",
      "Saved: novel_views_png/view_009.png\n",
      "Saved: novel_views_png/view_010.png\n",
      "Saved: novel_views_png/view_011.png\n",
      "Saved: novel_views_png/view_012.png\n",
      "Saved: novel_views_png/view_013.png\n",
      "Saved: novel_views_png/view_014.png\n",
      "Saved: novel_views_png/view_015.png\n",
      "Saved: novel_views_png/view_016.png\n",
      "Saved: novel_views_png/view_017.png\n",
      "Saved: novel_views_png/view_018.png\n",
      "Saved: novel_views_png/view_019.png\n",
      "Saved: novel_views_png/view_020.png\n",
      "Saved: novel_views_png/view_021.png\n",
      "Saved: novel_views_png/view_022.png\n",
      "Saved: novel_views_png/view_023.png\n",
      "Saved: novel_views_png/view_024.png\n",
      "Saved: novel_views_png/view_025.png\n",
      "Saved: novel_views_png/view_026.png\n",
      "Saved: novel_views_png/view_027.png\n",
      "Saved: novel_views_png/view_028.png\n",
      "Saved: novel_views_png/view_029.png\n",
      "Saved: novel_views_png/view_030.png\n",
      "Saved: novel_views_png/view_031.png\n",
      "All frames saved to folder: novel_views_png\n",
      "NOTE: Create video locally by stitching PNGs together.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "def save_image_tensor(img_tensor, path):\n",
    "    \"\"\"\n",
    "    img_tensor: (H*W, 3) in [0,1], torch float32\n",
    "    Saves as PNG using torchvision (safe, no numpy needed)\n",
    "    \"\"\"\n",
    "    Hc = int(math.sqrt(img_tensor.shape[0]))\n",
    "    img = img_tensor.reshape(Hc, Hc, 3).permute(2, 0, 1)  # CHW for torchvision\n",
    "\n",
    "    # clamp & convert\n",
    "    img = torch.clamp(img, 0.0, 1.0)\n",
    "\n",
    "    try:\n",
    "        torchvision.utils.save_image(img, path)  # Saves .png\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(\"PNG save failed:\", e)\n",
    "        torch.save(img_tensor.cpu(), path + \".pt\")  # fallback\n",
    "        return False\n",
    "\n",
    "\n",
    "frames = []\n",
    "n_frames = 32\n",
    "origins_eval, lookats_eval, ups_eval = generate_cameras(n_frames)\n",
    "\n",
    "os.makedirs(\"novel_views_png\", exist_ok=True)\n",
    "\n",
    "for i in range(n_frames):\n",
    "    r = render_novel_view(\n",
    "        model,\n",
    "        origins_eval[i],\n",
    "        lookats_eval[i],\n",
    "        ups_eval[i],\n",
    "        40, H, W,\n",
    "        0.1, 2.0,\n",
    "        128,\n",
    "        pe_pos, pe_dir\n",
    "    )\n",
    "    frames.append(r.cpu())\n",
    "\n",
    "    save_path = f\"novel_views_png/view_{i:03d}.png\"\n",
    "    save_image_tensor(r.cpu(), save_path)\n",
    "    print(\"Saved:\", save_path)\n",
    "\n",
    "print(\"All frames saved to folder: novel_views_png\")\n",
    "print(\"NOTE: Create video locally by stitching PNGs together.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420e1a95",
   "metadata": {},
   "source": [
    "## Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58531ee0",
   "metadata": {},
   "source": [
    "Neural Radiance Fields (NeRF) provide a compact, differentiable representation of 3D appearance by mapping\n",
    "continuous 3D coordinates and viewing directions to emitted color and volumetric density using a multi-layer\n",
    "perceptron (MLP). This approach implicitly encodes geometry—rather than storing explicit surfaces—so geometry\n",
    "emerges from volumetric density predicted across sampled points along camera rays.\n",
    "\n",
    "This simplified implementation builds an MLP that receives positional encoded 3D coordinates and view\n",
    "direction encodings. Positional encoding maps low-dimensional inputs into a higher-frequency space using\n",
    "sine/cosine bases, enabling the network to represent high-frequency variation (sharp edges, fine geometry)\n",
    "despite using smooth activation functions. The MLP uses a density head (sigma) and a color head; the density\n",
    "controls opacity along sampled ray segments while the color head conditions on both spatial features and\n",
    "view direction to reproduce view-dependent effects (specular highlights).\n",
    "\n",
    "Volume rendering integrates density-weighted colors along a ray: points are sampled between near and far\n",
    "bounds; network outputs color and density; weights are computed from density and distances between samples;\n",
    "and the weighted sum yields the pixel color and depth estimate. Training supervises rendered colors against\n",
    "ground-truth images for many camera poses. The optimization drives the MLP to assign high density where\n",
    "rays consistently intersect an object's surface thus implicitly reconstructing the geometry, while view-dependent\n",
    "color allows realistic appearance variations.\n",
    "\n",
    "Trade-offs: sample count, model width, and training views govern the speed-vs-quality trade-off. More samples\n",
    "and larger networks improve fidelity but increase compute and memory. Positional encoding frequency boosts\n",
    "detail but can lead to sneaking high-frequency artifacts or slower convergence. Practical speedups include\n",
    "coarse-to-fine sampling strategies, hierarchical sampling, smaller inference resolutions, caching learned\n",
    "features, or using smaller but efficient architectures (e.g., instant-ngp hash encodings or voxel-grid\n",
    "priors) for real-time rendering. For quality, increasing training views and iterations yields better geometry\n",
    "and view synthesis.\n",
    "\n",
    "In this assignment, we used a synthetic sphere dataset to validate the whole pipeline quickly. Despite the\n",
    "dataset's simplicity compared with complex natural scenes, the model demonstrates how continuous volumetric\n",
    "representations capture both geometry (density) and appearance (view-conditioned color). The provided code\n",
    "is modular — replace the synthetic dataset with Blender renders and poses for real scenes. Quantitative metrics\n",
    "(PSNR/SSIM and LPIPS if available) and saved renders are provided to evaluate and compare different speed/\n",
    "quality configurations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168ddde8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
