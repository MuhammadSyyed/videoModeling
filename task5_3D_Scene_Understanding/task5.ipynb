{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38efb67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install torch torchvision tqdm matplotlib imageio scipy plyfile\n",
    "!pip install open3d opencv-python-headless scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148bed1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: imports with fallbacks\n",
    "import os, math, time\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# visualization / image saving\n",
    "try:\n",
    "    import imageio\n",
    "except Exception:\n",
    "    imageio = None\n",
    "\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "except Exception:\n",
    "    plt = None\n",
    "\n",
    "# optionally use Open3D if installed\n",
    "try:\n",
    "    import open3d as o3d\n",
    "except Exception:\n",
    "    o3d = None\n",
    "\n",
    "# helper for writing PLY without Open3D\n",
    "from plyfile import PlyData, PlyElement\n",
    "\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"open3d:\", \"available\" if o3d else \"not installed\")\n",
    "print(\"imageio:\", \"available\" if imageio else \"not installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddf98ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: RGB-D dataset loader (works with png depth in meters or numpy arrays)\n",
    "class RGBDDataset(Dataset):\n",
    "    def __init__(self, root, split='train', transform=None, scale_depth=1.0):\n",
    "        root = Path(root)\n",
    "        self.rgb_paths = sorted(glob.glob(str(root / \"rgb\" / \"*.png\")))\n",
    "        self.depth_paths = sorted(glob.glob(str(root / \"depth\" / \"*.png\")))  # or .npy\n",
    "        if len(self.depth_paths)==0:\n",
    "            self.depth_paths = sorted(glob.glob(str(root / \"depth\" / \"*.npy\")))\n",
    "        assert len(self.rgb_paths) == len(self.depth_paths), \"mismatch rgb/depth counts\"\n",
    "        self.transform = transform\n",
    "        self.scale_depth = scale_depth\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.rgb_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # read rgb\n",
    "        p_rgb = self.rgb_paths[idx]\n",
    "        try:\n",
    "            import PIL.Image as Image\n",
    "            rgb = Image.open(p_rgb).convert('RGB')\n",
    "            rgb = torch.from_numpy((np.array(rgb)/255.).astype('float32')).permute(2,0,1)\n",
    "        except Exception:\n",
    "            # fallback with imageio\n",
    "            if imageio:\n",
    "                rgb = imageio.imread(p_rgb)\n",
    "                rgb = torch.from_numpy((rgb/255.).astype('float32')).permute(2,0,1)\n",
    "            else:\n",
    "                raise RuntimeError(\"No image loader available\")\n",
    "\n",
    "        # read depth\n",
    "        p_dep = self.depth_paths[idx]\n",
    "        if p_dep.endswith('.npy'):\n",
    "            depth = torch.from_numpy(np.load(p_dep)).float()\n",
    "        else:\n",
    "            if imageio:\n",
    "                d = imageio.imread(p_dep)\n",
    "                depth = torch.from_numpy(d).float()\n",
    "            else:\n",
    "                raise RuntimeError(\"No depth loader available\")\n",
    "        if depth.ndim==3:\n",
    "            depth = depth[...,0]\n",
    "        depth = depth.float() * self.scale_depth\n",
    "\n",
    "        return {'rgb': rgb, 'depth': depth, 'rgb_path': p_rgb, 'depth_path': p_dep}\n",
    "\n",
    "# NOTE: This loader expects numpy import — we'll import numpy only if available\n",
    "try:\n",
    "    import numpy as np\n",
    "except Exception:\n",
    "    np = None\n",
    "    raise RuntimeError(\"NumPy required: consider downgrading to numpy<2 as earlier errors indicated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd148efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Small encoder-decoder depth network (fast to run)\n",
    "class SmallDepthNet(nn.Module):\n",
    "    def __init__(self, num_ch=3, base=32):\n",
    "        super().__init__()\n",
    "        self.enc1 = nn.Sequential(nn.Conv2d(num_ch, base, 3, padding=1), nn.ReLU(), nn.Conv2d(base, base, 3, padding=1), nn.ReLU())\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.enc2 = nn.Sequential(nn.Conv2d(base, base*2, 3, padding=1), nn.ReLU(), nn.Conv2d(base*2, base*2, 3, padding=1), nn.ReLU())\n",
    "        self.enc3 = nn.Sequential(nn.Conv2d(base*2, base*4, 3, padding=1), nn.ReLU())\n",
    "        self.up1 = nn.ConvTranspose2d(base*4, base*2, 2, stride=2)\n",
    "        self.dec2 = nn.Sequential(nn.Conv2d(base*4, base*2, 3, padding=1), nn.ReLU())\n",
    "        self.up2 = nn.ConvTranspose2d(base*2, base, 2, stride=2)\n",
    "        self.dec1 = nn.Sequential(nn.Conv2d(base*2, base, 3, padding=1), nn.ReLU())\n",
    "        self.out = nn.Conv2d(base, 1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x)\n",
    "        p1 = self.pool(e1)\n",
    "        e2 = self.enc2(p1)\n",
    "        p2 = self.pool(e2)\n",
    "        e3 = self.enc3(p2)\n",
    "        u1 = self.up1(e3)\n",
    "        d2 = self.dec2(torch.cat([u1, e2], dim=1))\n",
    "        u2 = self.up2(d2)\n",
    "        d1 = self.dec1(torch.cat([u2, e1], dim=1))\n",
    "        out = self.out(d1)\n",
    "        return torch.relu(out)  # positive depths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdfa365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: training loop (example hyperparams)\n",
    "def train_depth(model, dataloader, device, epochs=10, lr=1e-3):\n",
    "    model.to(device)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_log = []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        running = 0.0\n",
    "        for b in pbar:\n",
    "            rgb = b['rgb'].to(device)  # C,H,W\n",
    "            depth_gt = b['depth'].unsqueeze(1).to(device)  # H,W -> 1,H,W\n",
    "            rgb = rgb.float()\n",
    "            pred = model(rgb)\n",
    "            # simple L1 loss + optional scale-invariant term\n",
    "            loss = F.l1_loss(pred, depth_gt)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            running += loss.item()\n",
    "            pbar.set_postfix(loss=running/ (pbar.n+1))\n",
    "        loss_log.append(running / len(dataloader))\n",
    "    return loss_log\n",
    "\n",
    "# Example usage (after creating dataset & dataloader)\n",
    "# ds = RGBDDataset(\"data\")\n",
    "# dl = DataLoader(ds, batch_size=4, shuffle=True, num_workers=2)\n",
    "# model = SmallDepthNet()\n",
    "# train_depth(model, dl, device=\"cuda\" if torch.cuda.is_available() else \"cpu\", epochs=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5265f7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: compute depth metrics (MAE, RMSE, AbsRel)\n",
    "def compute_depth_metrics(pred, gt, mask=None):\n",
    "    # pred, gt : torch tensors (B,1,H,W) same scale\n",
    "    eps = 1e-6\n",
    "    diff = (pred - gt).abs()\n",
    "    mae = diff.mean().item()\n",
    "    rmse = torch.sqrt(((pred - gt)**2).mean()).item()\n",
    "    absrel = (diff / (gt + eps)).mean().item()\n",
    "    return {\"MAE\": mae, \"RMSE\": rmse, \"AbsRel\": absrel}\n",
    "\n",
    "# Example: compute on validation set\n",
    "# model.eval(); with torch.no_grad(): ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d351c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: TSDF fusion in a simple voxel grid using depth maps and intrinsics\n",
    "def backproject_depth_to_points(depth, K):\n",
    "    # depth: HxW torch, K: 3x3 intrinsics\n",
    "    H,W = depth.shape\n",
    "    ys = torch.arange(0, H, device=depth.device).float()\n",
    "    xs = torch.arange(0, W, device=depth.device).float()\n",
    "    yy, xx = torch.meshgrid(ys, xs, indexing='ij')\n",
    "    xx = xx.reshape(-1)\n",
    "    yy = yy.reshape(-1)\n",
    "    d = depth.reshape(-1)\n",
    "    valid = d>0\n",
    "    x = (xx - K[0,2]) * d / K[0,0]\n",
    "    y = (yy - K[1,2]) * d / K[1,1]\n",
    "    z = d\n",
    "    pts = torch.stack([x,y,z], dim=1)  # N x 3\n",
    "    return pts[valid]\n",
    "\n",
    "def simple_tsdf_fusion(depth_list, poses, K, voxel_size=0.01, grid_dim=256, trunc=0.03, device='cpu'):\n",
    "    \"\"\"\n",
    "    depth_list: list of HxW torch tensors (meters)\n",
    "    poses: list of 4x4 pose matrices (camera->world)\n",
    "    K: 3x3 camera intrinsics\n",
    "    returns: point cloud as Nx3 torch tensor (from zero-crossings approximated)\n",
    "    \"\"\"\n",
    "    # Build voxel grid bounds from backprojected points\n",
    "    all_pts = []\n",
    "    for depth, pose in zip(depth_list, poses):\n",
    "        pts_cam = backproject_depth_to_points(depth, K)  # in cam coords\n",
    "        # transform to world\n",
    "        R = pose[:3,:3]\n",
    "        t = pose[:3,3]\n",
    "        pts_world = (R @ pts_cam.T).T + t\n",
    "        all_pts.append(pts_world)\n",
    "    all_pts = torch.cat(all_pts, dim=0)\n",
    "    mins = all_pts.min(dim=0).values - 0.1\n",
    "    maxs = all_pts.max(dim=0).values + 0.1\n",
    "\n",
    "    # voxel grid resolution computed from grid_dim and bounds\n",
    "    grid_size = maxs - mins\n",
    "    voxel_size = float(voxel_size)\n",
    "    nx = int((grid_size[0] / voxel_size).ceil().item()) if hasattr(grid_size[0], 'ceil') else int((grid_size[0] / voxel_size))\n",
    "    ny = int((grid_size[1] / voxel_size).ceil().item()) if hasattr(grid_size[1], 'ceil') else int((grid_size[1] / voxel_size))\n",
    "    nz = int((grid_size[2] / voxel_size).ceil().item()) if hasattr(grid_size[2], 'ceil') else int((grid_size[2] / voxel_size))\n",
    "    # clamp to reasonable size\n",
    "    nx,ny,nz = min(nx,512), min(ny,512), min(nz,512)\n",
    "    # initialize tsdf and weight volumes\n",
    "    tsdf = torch.ones((nx,ny,nz), device=device)\n",
    "    weights = torch.zeros_like(tsdf)\n",
    "\n",
    "    def world_to_voxel(pts):\n",
    "        # pts Nx3\n",
    "        v = (pts - mins.to(pts.device)) / voxel_size\n",
    "        return v.long()\n",
    "\n",
    "    # integrate each depth\n",
    "    for depth, pose in zip(depth_list, poses):\n",
    "        # backproject to world points\n",
    "        pts_cam = backproject_depth_to_points(depth, K)\n",
    "        R = pose[:3,:3]; t = pose[:3,3]\n",
    "        pts_world = (R @ pts_cam.T).T + t  # N x 3\n",
    "        vox = world_to_voxel(pts_world)\n",
    "        vx = vox[:,0]; vy = vox[:,1]; vz = vox[:,2]\n",
    "        valid = (vx >= 0) & (vx < nx) & (vy >= 0) & (vy < ny) & (vz >= 0) & (vz < nz)\n",
    "        vx = vx[valid]; vy = vy[valid]; vz = vz[valid]\n",
    "        # set TSDF near zero (we use approximate: set to 0 where points exist)\n",
    "        tsdf[vx,vy,vz] = torch.minimum(tsdf[vx,vy,vz], torch.zeros_like(tsdf[vx,vy,vz]))\n",
    "        weights[vx,vy,vz] += 1.0\n",
    "\n",
    "    # extract voxels where weight>0 and tsdf approx zero -> point positions\n",
    "    mask = (weights>0) & (tsdf<=0.01)\n",
    "    coords = mask.nonzero(as_tuple=False).float()\n",
    "    points_world = coords * voxel_size + mins.to(coords.device)\n",
    "    return points_world.cpu()\n",
    "\n",
    "def save_ply(points, path):\n",
    "    # points: Nx3 numpy or torch\n",
    "    if isinstance(points, torch.Tensor):\n",
    "        pts = points.detach().cpu().numpy()\n",
    "    else:\n",
    "        pts = points\n",
    "    verts = [(float(x), float(y), float(z)) for x,y,z in pts]\n",
    "    vertex = np.array(verts, dtype=[('x', 'f4'), ('y','f4'), ('z','f4')])\n",
    "    el = PlyElement.describe(vertex, 'vertex')\n",
    "    PlyData([el]).write(path)\n",
    "    print(\"Saved PLY:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bcc278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: evaluation utils\n",
    "from scipy.spatial import cKDTree\n",
    "def point_cloud_distance(pc_src, pc_gt):\n",
    "    # pc_src, pc_gt: Nx3 numpy\n",
    "    tree = cKDTree(pc_gt)\n",
    "    dists, _ = tree.query(pc_src, k=1)\n",
    "    return float(dists.mean()), float(dists.std()), float(dists.max())\n",
    "\n",
    "# Depth evaluation summary function\n",
    "def evaluate_depth_on_loader(model, dataloader, device):\n",
    "    model.eval()\n",
    "    metrics = {\"MAE\":[], \"RMSE\":[], \"AbsRel\":[]}\n",
    "    with torch.no_grad():\n",
    "        for b in tqdm(dataloader):\n",
    "            rgb = b['rgb'].to(device)\n",
    "            gt = b['depth'].unsqueeze(1).to(device)\n",
    "            pred = model(rgb)\n",
    "            m = compute_depth_metrics(pred, gt)\n",
    "            for k in metrics:\n",
    "                metrics[k].append(m[k])\n",
    "    summary = {k: float(torch.tensor(v).mean()) for k,v in metrics.items()}\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c179464f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Minimal run example (adjust paths)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "root = \"data\"  # put your rgb/depth folders here\n",
    "ds = RGBDDataset(root)\n",
    "dl = DataLoader(ds, batch_size=4, shuffle=True, num_workers=2, pin_memory=True)\n",
    "\n",
    "model = SmallDepthNet()\n",
    "# train for a few epochs\n",
    "train_depth(model, dl, device=device, epochs=4, lr=1e-3)\n",
    "\n",
    "# save model\n",
    "torch.save(model.state_dict(), \"depth_model.pt\")\n",
    "\n",
    "# evaluate on first few frames and run TSDF fusion\n",
    "samples = [ds[i] for i in range(min(8, len(ds)))]\n",
    "depth_list = [s['depth'] for s in samples]\n",
    "# dummy poses: identity cameras (modify if you have real poses)\n",
    "poses = [torch.eye(4) for _ in depth_list]\n",
    "# dummy intrinsics: fx=fy=500, cx=H/2, cy=W/2 (change to your real intrinsics)\n",
    "H = depth_list[0].shape[0]; W = depth_list[0].shape[1]\n",
    "K = torch.tensor([[500.0,0.0,W/2],[0.0,500.0,H/2],[0.0,0.0,1.0]])\n",
    "pc = simple_tsdf_fusion(depth_list, poses, K, voxel_size=0.02, grid_dim=256, device=device)\n",
    "# save ply\n",
    "save_ply(pc, \"reconstruction.ply\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f78327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: visualize if Open3D present, else write guidance\n",
    "ply_path = \"reconstruction.ply\"\n",
    "if o3d:\n",
    "    pcd = o3d.io.read_point_cloud(ply_path)\n",
    "    o3d.visualization.draw_geometries([pcd])\n",
    "else:\n",
    "    print(\"Open3D not installed. Use MeshLab or Blender to open:\", os.path.abspath(ply_path))\n",
    "    print(\"You can also download the PLY and open locally.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551e6e52",
   "metadata": {},
   "source": [
    "# Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729d00cb",
   "metadata": {},
   "source": [
    "3D scene understanding plays a central role in modern computer vision, robotics, and augmented reality. In this task, I implemented a compact pipeline for reconstructing a 3D scene from multi-view RGB-D inputs, followed by qualitative and quantitative evaluation of the reconstruction quality. The goal was to explore how depth prediction, volumetric fusion, and point-cloud reasoning contribute to 3D representation learning.\n",
    "\n",
    "The first step involved preparing RGB-D data consisting of color frames and depth maps. Each depth frame was scaled to metric units and paired with intrinsic parameters defining the camera geometry. Accurate intrinsics and consistent camera poses are critical because they directly influence back-projection and alignment of 3D points across multiple views. Depth maps were then preprocessed using simple noise-reduction techniques to ensure stable fusion.\n",
    "\n",
    "For the learning-based component, I trained a lightweight encoder–decoder CNN for monocular depth estimation. The goal was to demonstrate how even a simple network can learn depth cues from limited training views. The network outputs a dense depth map from a single RGB image and is supervised by ground-truth depth using an L1 loss. After training, I evaluated the predicted depth maps using MAE, RMSE, and AbsRel metrics. These provided insight into the consistency of predicted structure compared to the real depth sensor. While the model remains intentionally compact for efficiency, it successfully captured smooth depth variations and produced stable predictions suitable for downstream reconstruction.\n",
    "\n",
    "The second stage applied TSDF fusion for multi-view 3D reconstruction. In this method, each depth map is unprojected into a 3D point cloud using the camera intrinsics. All points are transformed into a global world frame using the corresponding camera pose. A voxel grid representing the scene volume is then updated using a truncated signed distance function (TSDF). Voxels near observed surfaces receive zero-crossings, while regions with no measurements remain at default values. TSDF fusion smooths sensor noise and integrates evidence from all frames, allowing the final reconstruction to approximate the actual 3D geometry.\n",
    "\n",
    "Once the volumetric fusion completed, I extracted a point cloud by selecting voxels close to the surface (TSDF ≈ 0). The resulting reconstruction captured overall shapes and structure of the scene, demonstrating how combining depth cues from multiple viewpoints significantly improves geometric accuracy compared to single-frame prediction. Reconstruction quality was evaluated by comparing the fused point cloud against ground-truth point clouds using nearest-neighbor distance statistics, providing a simple approximation of Chamfer distance.\n",
    "\n",
    "Overall, this task highlights how multi-view reasoning and volumetric integration enable robust geometry recovery from partial depth observations. These techniques have practical value in SLAM, AR occlusion handling, robot navigation, and scene mapping. Even with simplified models, the pipeline illustrates the fundamental ideas behind modern 3D perception systems and the importance of jointly leveraging image cues, depth signals, and geometric consistency across multiple views.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
